{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network\n",
    "\n",
    "In this notebook, we will train the CNN-RNN model for Image captioning\n",
    "\n",
    "CNN [ResNet](https://arxiv.org/pdf/1512.03385.pdf) model is used for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/farid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/farid/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/farid/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from model import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farid farid\n"
     ]
    }
   ],
   "source": [
    "HOME: str = os.getenv('HOME') # echo $HOME\n",
    "USER: str = os.getenv('USER') # echo $USER\n",
    "print(HOME, USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val2017.zip', 'train2017.zip', 'images', 'annotations_trainval2017.zip', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# dataset dir path\n",
    "cocoapi_dir = os.path.join(\"/scratch/project_2004072/IMG_Captioning\", \"MS_COCO\") if USER==\"alijanif\" else os.path.join(HOME, \"datasets/MS_COCO\")\n",
    "folders = [folder for folder in os.listdir(cocoapi_dir)]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1  # training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591753/591753 [00:23<00:00, 25295.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder and RNN Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  11543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/farid/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:28<00:00, 3.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [20/4624], Loss: 4.8973, Perplexity: 133.9271\n",
      "Epoch [1/1], Step [40/4624], Loss: 4.9032, Perplexity: 134.7243\n",
      "Epoch [1/1], Step [60/4624], Loss: 4.3973, Perplexity: 81.2294\n",
      "Epoch [1/1], Step [80/4624], Loss: 3.9979, Perplexity: 54.4858\n",
      "Epoch [1/1], Step [100/4624], Loss: 3.9218, Perplexity: 50.4938\n",
      "Epoch [1/1], Step [120/4624], Loss: 4.0951, Perplexity: 60.0455\n",
      "Epoch [1/1], Step [140/4624], Loss: 3.6245, Perplexity: 37.5077\n",
      "Epoch [1/1], Step [160/4624], Loss: 3.9668, Perplexity: 52.8144\n",
      "Epoch [1/1], Step [180/4624], Loss: 3.4522, Perplexity: 31.5700\n",
      "Epoch [1/1], Step [200/4624], Loss: 3.4176, Perplexity: 30.4957\n",
      "Epoch [1/1], Step [220/4624], Loss: 3.6445, Perplexity: 38.2623\n",
      "Epoch [1/1], Step [240/4624], Loss: 3.8861, Perplexity: 48.7210\n",
      "Epoch [1/1], Step [260/4624], Loss: 3.5298, Perplexity: 34.1169\n",
      "Epoch [1/1], Step [280/4624], Loss: 3.4250, Perplexity: 30.7239\n",
      "Epoch [1/1], Step [300/4624], Loss: 3.1883, Perplexity: 24.2463\n",
      "Epoch [1/1], Step [320/4624], Loss: 3.1663, Perplexity: 23.7195\n",
      "Epoch [1/1], Step [340/4624], Loss: 3.2092, Perplexity: 24.7586\n",
      "Epoch [1/1], Step [360/4624], Loss: 3.2021, Perplexity: 24.5850\n",
      "Epoch [1/1], Step [380/4624], Loss: 3.6553, Perplexity: 38.6804\n",
      "Epoch [1/1], Step [400/4624], Loss: 3.4759, Perplexity: 32.3268\n",
      "Epoch [1/1], Step [420/4624], Loss: 3.3213, Perplexity: 27.6962\n",
      "Epoch [1/1], Step [440/4624], Loss: 3.9322, Perplexity: 51.0172\n",
      "Epoch [1/1], Step [460/4624], Loss: 3.4384, Perplexity: 31.1376\n",
      "Epoch [1/1], Step [480/4624], Loss: 3.6501, Perplexity: 38.4790\n",
      "Epoch [1/1], Step [500/4624], Loss: 3.1013, Perplexity: 22.2261\n",
      "Epoch [1/1], Step [520/4624], Loss: 2.9714, Perplexity: 19.5195\n",
      "Epoch [1/1], Step [540/4624], Loss: 3.2841, Perplexity: 26.6853\n",
      "Epoch [1/1], Step [560/4624], Loss: 3.1665, Perplexity: 23.7254\n",
      "Epoch [1/1], Step [580/4624], Loss: 3.0282, Perplexity: 20.6609\n",
      "Epoch [1/1], Step [600/4624], Loss: 2.8839, Perplexity: 17.8844\n",
      "Epoch [1/1], Step [620/4624], Loss: 3.1163, Perplexity: 22.5633\n",
      "Epoch [1/1], Step [640/4624], Loss: 2.7322, Perplexity: 15.3667\n",
      "Epoch [1/1], Step [660/4624], Loss: 3.0517, Perplexity: 21.1504\n",
      "Epoch [1/1], Step [680/4624], Loss: 2.6825, Perplexity: 14.6213\n",
      "Epoch [1/1], Step [700/4624], Loss: 2.8352, Perplexity: 17.0346\n",
      "Epoch [1/1], Step [720/4624], Loss: 3.1155, Perplexity: 22.5437\n",
      "Epoch [1/1], Step [740/4624], Loss: 2.6723, Perplexity: 14.4736\n",
      "Epoch [1/1], Step [760/4624], Loss: 2.5869, Perplexity: 13.2879\n",
      "Epoch [1/1], Step [780/4624], Loss: 2.6905, Perplexity: 14.7394\n",
      "Epoch [1/1], Step [800/4624], Loss: 2.7619, Perplexity: 15.8296\n",
      "Epoch [1/1], Step [820/4624], Loss: 2.5253, Perplexity: 12.4952\n",
      "Epoch [1/1], Step [840/4624], Loss: 2.7143, Perplexity: 15.0938\n",
      "Epoch [1/1], Step [860/4624], Loss: 3.2540, Perplexity: 25.8928\n",
      "Epoch [1/1], Step [880/4624], Loss: 2.6003, Perplexity: 13.4672\n",
      "Epoch [1/1], Step [900/4624], Loss: 2.7848, Perplexity: 16.1974\n",
      "Epoch [1/1], Step [920/4624], Loss: 2.4231, Perplexity: 11.2811\n",
      "Epoch [1/1], Step [940/4624], Loss: 3.0118, Perplexity: 20.3237\n",
      "Epoch [1/1], Step [960/4624], Loss: 2.8136, Perplexity: 16.6704\n",
      "Epoch [1/1], Step [980/4624], Loss: 2.7690, Perplexity: 15.9428\n",
      "Epoch [1/1], Step [1000/4624], Loss: 2.6428, Perplexity: 14.0521\n",
      "Epoch [1/1], Step [1020/4624], Loss: 2.6359, Perplexity: 13.9554\n",
      "Epoch [1/1], Step [1040/4624], Loss: 2.5674, Perplexity: 13.0315\n",
      "Epoch [1/1], Step [1060/4624], Loss: 2.5830, Perplexity: 13.2367\n",
      "Epoch [1/1], Step [1080/4624], Loss: 2.9496, Perplexity: 19.0989\n",
      "Epoch [1/1], Step [1100/4624], Loss: 2.5337, Perplexity: 12.6001\n",
      "Epoch [1/1], Step [1120/4624], Loss: 2.4816, Perplexity: 11.9606\n",
      "Epoch [1/1], Step [1140/4624], Loss: 2.3998, Perplexity: 11.0205\n",
      "Epoch [1/1], Step [1160/4624], Loss: 2.4818, Perplexity: 11.9625\n",
      "Epoch [1/1], Step [1180/4624], Loss: 2.4230, Perplexity: 11.2802\n",
      "Epoch [1/1], Step [1200/4624], Loss: 2.4403, Perplexity: 11.4765\n",
      "Epoch [1/1], Step [1220/4624], Loss: 2.4152, Perplexity: 11.1919\n",
      "Epoch [1/1], Step [1240/4624], Loss: 2.5971, Perplexity: 13.4241\n"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validating the Model using Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = val_get_loader(\n",
    "    transform=transform_test, \n",
    "    mode=\"valid\", \n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file = f\"encoder_{num_epochs}_nEpochs.pkl\"\n",
    "decoder_file = f\"decoder_{num_epochs}_nEpochs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad bleu score with only 3 epochs!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
