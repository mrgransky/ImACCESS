{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Network\n",
    "\n",
    "In this notebook, we will train the CNN-RNN model for Image captioning\n",
    "\n",
    "CNN [ResNet](https://arxiv.org/pdf/1512.03385.pdf) model is used for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from model import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farid farid\n"
     ]
    }
   ],
   "source": [
    "HOME: str = os.getenv('HOME') # echo $HOME\n",
    "USER: str = os.getenv('USER') # echo $USER\n",
    "print(HOME, USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'annotations']\n"
     ]
    }
   ],
   "source": [
    "# dataset dir path\n",
    "cocoapi_dir = os.path.join(\"/scratch/project_2004072/IMG_Captioning\", \"MS_COCO\") if USER==\"alijanif\" else os.path.join(HOME, \"datasets/MS_COCO\")\n",
    "folders = [folder for folder in os.listdir(cocoapi_dir)]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1  # training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 200  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "models_dir = \"models\"\n",
    "encoder_fname = f\"encoder_{num_epochs}_nEpochs.pkl\"\n",
    "decoder_fname = f\"decoder_{num_epochs}_nEpochs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591753/591753 [00:23<00:00, 25295.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Encoder and RNN Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  11543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/farid/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:28<00:00, 3.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4624\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [20/4624], Loss: 4.8973, Perplexity: 133.9271\n",
      "Epoch [1/1], Step [40/4624], Loss: 4.9032, Perplexity: 134.7243\n",
      "Epoch [1/1], Step [60/4624], Loss: 4.3973, Perplexity: 81.2294\n",
      "Epoch [1/1], Step [80/4624], Loss: 3.9979, Perplexity: 54.4858\n",
      "Epoch [1/1], Step [100/4624], Loss: 3.9218, Perplexity: 50.4938\n",
      "Epoch [1/1], Step [120/4624], Loss: 4.0951, Perplexity: 60.0455\n",
      "Epoch [1/1], Step [140/4624], Loss: 3.6245, Perplexity: 37.5077\n",
      "Epoch [1/1], Step [160/4624], Loss: 3.9668, Perplexity: 52.8144\n",
      "Epoch [1/1], Step [180/4624], Loss: 3.4522, Perplexity: 31.5700\n",
      "Epoch [1/1], Step [200/4624], Loss: 3.4176, Perplexity: 30.4957\n",
      "Epoch [1/1], Step [220/4624], Loss: 3.6445, Perplexity: 38.2623\n",
      "Epoch [1/1], Step [240/4624], Loss: 3.8861, Perplexity: 48.7210\n",
      "Epoch [1/1], Step [260/4624], Loss: 3.5298, Perplexity: 34.1169\n",
      "Epoch [1/1], Step [280/4624], Loss: 3.4250, Perplexity: 30.7239\n",
      "Epoch [1/1], Step [300/4624], Loss: 3.1883, Perplexity: 24.2463\n",
      "Epoch [1/1], Step [320/4624], Loss: 3.1663, Perplexity: 23.7195\n",
      "Epoch [1/1], Step [340/4624], Loss: 3.2092, Perplexity: 24.7586\n",
      "Epoch [1/1], Step [360/4624], Loss: 3.2021, Perplexity: 24.5850\n",
      "Epoch [1/1], Step [380/4624], Loss: 3.6553, Perplexity: 38.6804\n",
      "Epoch [1/1], Step [400/4624], Loss: 3.4759, Perplexity: 32.3268\n",
      "Epoch [1/1], Step [420/4624], Loss: 3.3213, Perplexity: 27.6962\n",
      "Epoch [1/1], Step [440/4624], Loss: 3.9322, Perplexity: 51.0172\n",
      "Epoch [1/1], Step [460/4624], Loss: 3.4384, Perplexity: 31.1376\n",
      "Epoch [1/1], Step [480/4624], Loss: 3.6501, Perplexity: 38.4790\n",
      "Epoch [1/1], Step [500/4624], Loss: 3.1013, Perplexity: 22.2261\n",
      "Epoch [1/1], Step [520/4624], Loss: 2.9714, Perplexity: 19.5195\n",
      "Epoch [1/1], Step [540/4624], Loss: 3.2841, Perplexity: 26.6853\n",
      "Epoch [1/1], Step [560/4624], Loss: 3.1665, Perplexity: 23.7254\n",
      "Epoch [1/1], Step [580/4624], Loss: 3.0282, Perplexity: 20.6609\n",
      "Epoch [1/1], Step [600/4624], Loss: 2.8839, Perplexity: 17.8844\n",
      "Epoch [1/1], Step [620/4624], Loss: 3.1163, Perplexity: 22.5633\n",
      "Epoch [1/1], Step [640/4624], Loss: 2.7322, Perplexity: 15.3667\n",
      "Epoch [1/1], Step [660/4624], Loss: 3.0517, Perplexity: 21.1504\n",
      "Epoch [1/1], Step [680/4624], Loss: 2.6825, Perplexity: 14.6213\n",
      "Epoch [1/1], Step [700/4624], Loss: 2.8352, Perplexity: 17.0346\n",
      "Epoch [1/1], Step [720/4624], Loss: 3.1155, Perplexity: 22.5437\n",
      "Epoch [1/1], Step [740/4624], Loss: 2.6723, Perplexity: 14.4736\n",
      "Epoch [1/1], Step [760/4624], Loss: 2.5869, Perplexity: 13.2879\n",
      "Epoch [1/1], Step [780/4624], Loss: 2.6905, Perplexity: 14.7394\n",
      "Epoch [1/1], Step [800/4624], Loss: 2.7619, Perplexity: 15.8296\n",
      "Epoch [1/1], Step [820/4624], Loss: 2.5253, Perplexity: 12.4952\n",
      "Epoch [1/1], Step [840/4624], Loss: 2.7143, Perplexity: 15.0938\n",
      "Epoch [1/1], Step [860/4624], Loss: 3.2540, Perplexity: 25.8928\n",
      "Epoch [1/1], Step [880/4624], Loss: 2.6003, Perplexity: 13.4672\n",
      "Epoch [1/1], Step [900/4624], Loss: 2.7848, Perplexity: 16.1974\n",
      "Epoch [1/1], Step [920/4624], Loss: 2.4231, Perplexity: 11.2811\n",
      "Epoch [1/1], Step [940/4624], Loss: 3.0118, Perplexity: 20.3237\n",
      "Epoch [1/1], Step [960/4624], Loss: 2.8136, Perplexity: 16.6704\n",
      "Epoch [1/1], Step [980/4624], Loss: 2.7690, Perplexity: 15.9428\n",
      "Epoch [1/1], Step [1000/4624], Loss: 2.6428, Perplexity: 14.0521\n",
      "Epoch [1/1], Step [1020/4624], Loss: 2.6359, Perplexity: 13.9554\n",
      "Epoch [1/1], Step [1040/4624], Loss: 2.5674, Perplexity: 13.0315\n",
      "Epoch [1/1], Step [1060/4624], Loss: 2.5830, Perplexity: 13.2367\n",
      "Epoch [1/1], Step [1080/4624], Loss: 2.9496, Perplexity: 19.0989\n",
      "Epoch [1/1], Step [1100/4624], Loss: 2.5337, Perplexity: 12.6001\n",
      "Epoch [1/1], Step [1120/4624], Loss: 2.4816, Perplexity: 11.9606\n",
      "Epoch [1/1], Step [1140/4624], Loss: 2.3998, Perplexity: 11.0205\n",
      "Epoch [1/1], Step [1160/4624], Loss: 2.4818, Perplexity: 11.9625\n",
      "Epoch [1/1], Step [1180/4624], Loss: 2.4230, Perplexity: 11.2802\n",
      "Epoch [1/1], Step [1200/4624], Loss: 2.4403, Perplexity: 11.4765\n",
      "Epoch [1/1], Step [1220/4624], Loss: 2.4152, Perplexity: 11.1919\n",
      "Epoch [1/1], Step [1240/4624], Loss: 2.5971, Perplexity: 13.4241\n",
      "Epoch [1/1], Step [1260/4624], Loss: 2.3826, Perplexity: 10.8330\n",
      "Epoch [1/1], Step [1280/4624], Loss: 2.5592, Perplexity: 12.9255\n",
      "Epoch [1/1], Step [1300/4624], Loss: 2.3867, Perplexity: 10.8778\n",
      "Epoch [1/1], Step [1320/4624], Loss: 2.3700, Perplexity: 10.6972\n",
      "Epoch [1/1], Step [1340/4624], Loss: 2.8380, Perplexity: 17.0810\n",
      "Epoch [1/1], Step [1360/4624], Loss: 2.4090, Perplexity: 11.1223\n",
      "Epoch [1/1], Step [1380/4624], Loss: 2.4072, Perplexity: 11.1033\n",
      "Epoch [1/1], Step [1400/4624], Loss: 2.6191, Perplexity: 13.7229\n",
      "Epoch [1/1], Step [1420/4624], Loss: 2.5108, Perplexity: 12.3146\n",
      "Epoch [1/1], Step [1440/4624], Loss: 2.4612, Perplexity: 11.7187\n",
      "Epoch [1/1], Step [1460/4624], Loss: 2.6183, Perplexity: 13.7126\n",
      "Epoch [1/1], Step [1480/4624], Loss: 2.2148, Perplexity: 9.1595\n",
      "Epoch [1/1], Step [1500/4624], Loss: 2.3532, Perplexity: 10.5187\n",
      "Epoch [1/1], Step [1520/4624], Loss: 2.3779, Perplexity: 10.7824\n",
      "Epoch [1/1], Step [1540/4624], Loss: 2.4869, Perplexity: 12.0238\n",
      "Epoch [1/1], Step [1560/4624], Loss: 2.3294, Perplexity: 10.2714\n",
      "Epoch [1/1], Step [1580/4624], Loss: 2.3544, Perplexity: 10.5319\n",
      "Epoch [1/1], Step [1600/4624], Loss: 2.3774, Perplexity: 10.7770\n",
      "Epoch [1/1], Step [1620/4624], Loss: 2.1181, Perplexity: 8.3152\n",
      "Epoch [1/1], Step [1640/4624], Loss: 2.2182, Perplexity: 9.1905\n",
      "Epoch [1/1], Step [1660/4624], Loss: 2.4489, Perplexity: 11.5753\n",
      "Epoch [1/1], Step [1680/4624], Loss: 2.3064, Perplexity: 10.0386\n",
      "Epoch [1/1], Step [1700/4624], Loss: 2.3445, Perplexity: 10.4281\n",
      "Epoch [1/1], Step [1720/4624], Loss: 2.6480, Perplexity: 14.1251\n",
      "Epoch [1/1], Step [1740/4624], Loss: 2.3325, Perplexity: 10.3034\n",
      "Epoch [1/1], Step [1760/4624], Loss: 2.5705, Perplexity: 13.0728\n",
      "Epoch [1/1], Step [1780/4624], Loss: 2.7030, Perplexity: 14.9237\n",
      "Epoch [1/1], Step [1800/4624], Loss: 2.3338, Perplexity: 10.3174\n",
      "Epoch [1/1], Step [1820/4624], Loss: 2.5128, Perplexity: 12.3393\n",
      "Epoch [1/1], Step [1840/4624], Loss: 2.3166, Perplexity: 10.1413\n",
      "Epoch [1/1], Step [1860/4624], Loss: 2.1627, Perplexity: 8.6948\n",
      "Epoch [1/1], Step [1880/4624], Loss: 2.6122, Perplexity: 13.6284\n",
      "Epoch [1/1], Step [1900/4624], Loss: 2.2981, Perplexity: 9.9554\n",
      "Epoch [1/1], Step [1920/4624], Loss: 2.3552, Perplexity: 10.5397\n",
      "Epoch [1/1], Step [1940/4624], Loss: 2.2156, Perplexity: 9.1671\n",
      "Epoch [1/1], Step [1960/4624], Loss: 3.0563, Perplexity: 21.2480\n",
      "Epoch [1/1], Step [1980/4624], Loss: 2.2834, Perplexity: 9.8101\n",
      "Epoch [1/1], Step [2000/4624], Loss: 2.2365, Perplexity: 9.3605\n",
      "Epoch [1/1], Step [2020/4624], Loss: 2.1337, Perplexity: 8.4463\n",
      "Epoch [1/1], Step [2040/4624], Loss: 2.2089, Perplexity: 9.1057\n",
      "Epoch [1/1], Step [2060/4624], Loss: 2.2948, Perplexity: 9.9221\n",
      "Epoch [1/1], Step [2080/4624], Loss: 2.2639, Perplexity: 9.6206\n",
      "Epoch [1/1], Step [2100/4624], Loss: 2.4039, Perplexity: 11.0666\n",
      "Epoch [1/1], Step [2120/4624], Loss: 2.6008, Perplexity: 13.4739\n",
      "Epoch [1/1], Step [2140/4624], Loss: 2.1719, Perplexity: 8.7753\n",
      "Epoch [1/1], Step [2160/4624], Loss: 2.8206, Perplexity: 16.7865\n",
      "Epoch [1/1], Step [2180/4624], Loss: 2.5001, Perplexity: 12.1839\n",
      "Epoch [1/1], Step [2200/4624], Loss: 2.6494, Perplexity: 14.1452\n",
      "Epoch [1/1], Step [2220/4624], Loss: 2.2714, Perplexity: 9.6925\n",
      "Epoch [1/1], Step [2240/4624], Loss: 2.2608, Perplexity: 9.5905\n",
      "Epoch [1/1], Step [2260/4624], Loss: 2.0805, Perplexity: 8.0088\n",
      "Epoch [1/1], Step [2280/4624], Loss: 3.1181, Perplexity: 22.6039\n",
      "Epoch [1/1], Step [2300/4624], Loss: 2.1486, Perplexity: 8.5727\n",
      "Epoch [1/1], Step [2320/4624], Loss: 2.1661, Perplexity: 8.7239\n",
      "Epoch [1/1], Step [2340/4624], Loss: 2.0967, Perplexity: 8.1394\n",
      "Epoch [1/1], Step [2360/4624], Loss: 2.9059, Perplexity: 18.2824\n",
      "Epoch [1/1], Step [2380/4624], Loss: 2.2633, Perplexity: 9.6144\n",
      "Epoch [1/1], Step [2400/4624], Loss: 2.8161, Perplexity: 16.7114\n",
      "Epoch [1/1], Step [2420/4624], Loss: 2.2421, Perplexity: 9.4135\n",
      "Epoch [1/1], Step [2440/4624], Loss: 2.3071, Perplexity: 10.0453\n",
      "Epoch [1/1], Step [2460/4624], Loss: 2.3613, Perplexity: 10.6048\n",
      "Epoch [1/1], Step [2480/4624], Loss: 2.2549, Perplexity: 9.5347\n",
      "Epoch [1/1], Step [2500/4624], Loss: 2.2191, Perplexity: 9.1992\n",
      "Epoch [1/1], Step [2520/4624], Loss: 2.2267, Perplexity: 9.2692\n",
      "Epoch [1/1], Step [2540/4624], Loss: 2.5983, Perplexity: 13.4408\n",
      "Epoch [1/1], Step [2560/4624], Loss: 2.1854, Perplexity: 8.8938\n",
      "Epoch [1/1], Step [2580/4624], Loss: 2.7040, Perplexity: 14.9393\n",
      "Epoch [1/1], Step [2600/4624], Loss: 2.1425, Perplexity: 8.5203\n",
      "Epoch [1/1], Step [2620/4624], Loss: 2.1378, Perplexity: 8.4810\n",
      "Epoch [1/1], Step [2640/4624], Loss: 2.3145, Perplexity: 10.1202\n",
      "Epoch [1/1], Step [2660/4624], Loss: 2.1147, Perplexity: 8.2875\n",
      "Epoch [1/1], Step [2680/4624], Loss: 2.0828, Perplexity: 8.0266\n",
      "Epoch [1/1], Step [2700/4624], Loss: 2.2510, Perplexity: 9.4972\n",
      "Epoch [1/1], Step [2720/4624], Loss: 2.1172, Perplexity: 8.3082\n",
      "Epoch [1/1], Step [2740/4624], Loss: 2.0652, Perplexity: 7.8869\n",
      "Epoch [1/1], Step [2760/4624], Loss: 2.4733, Perplexity: 11.8612\n",
      "Epoch [1/1], Step [2780/4624], Loss: 2.1487, Perplexity: 8.5738\n",
      "Epoch [1/1], Step [2800/4624], Loss: 2.2331, Perplexity: 9.3290\n",
      "Epoch [1/1], Step [2820/4624], Loss: 2.2093, Perplexity: 9.1094\n",
      "Epoch [1/1], Step [2840/4624], Loss: 2.1825, Perplexity: 8.8682\n",
      "Epoch [1/1], Step [2860/4624], Loss: 2.1972, Perplexity: 9.0001\n",
      "Epoch [1/1], Step [2880/4624], Loss: 2.4203, Perplexity: 11.2488\n",
      "Epoch [1/1], Step [2900/4624], Loss: 2.1933, Perplexity: 8.9645\n",
      "Epoch [1/1], Step [2920/4624], Loss: 2.1635, Perplexity: 8.7017\n",
      "Epoch [1/1], Step [2940/4624], Loss: 2.0137, Perplexity: 7.4906\n",
      "Epoch [1/1], Step [2960/4624], Loss: 2.1824, Perplexity: 8.8674\n",
      "Epoch [1/1], Step [2980/4624], Loss: 2.2480, Perplexity: 9.4687\n",
      "Epoch [1/1], Step [3000/4624], Loss: 2.2618, Perplexity: 9.6003\n",
      "Epoch [1/1], Step [3020/4624], Loss: 2.1914, Perplexity: 8.9480\n",
      "Epoch [1/1], Step [3040/4624], Loss: 2.4237, Perplexity: 11.2876\n",
      "Epoch [1/1], Step [3060/4624], Loss: 2.6780, Perplexity: 14.5557\n",
      "Epoch [1/1], Step [3080/4624], Loss: 2.0908, Perplexity: 8.0917\n",
      "Epoch [1/1], Step [3100/4624], Loss: 2.2032, Perplexity: 9.0540\n",
      "Epoch [1/1], Step [3120/4624], Loss: 2.2013, Perplexity: 9.0372\n",
      "Epoch [1/1], Step [3140/4624], Loss: 2.1538, Perplexity: 8.6171\n",
      "Epoch [1/1], Step [3160/4624], Loss: 2.0397, Perplexity: 7.6885\n",
      "Epoch [1/1], Step [3180/4624], Loss: 2.1928, Perplexity: 8.9601\n",
      "Epoch [1/1], Step [3200/4624], Loss: 2.1280, Perplexity: 8.3976\n",
      "Epoch [1/1], Step [3220/4624], Loss: 2.1419, Perplexity: 8.5156\n",
      "Epoch [1/1], Step [3240/4624], Loss: 2.4142, Perplexity: 11.1805\n",
      "Epoch [1/1], Step [3260/4624], Loss: 2.0566, Perplexity: 7.8195\n",
      "Epoch [1/1], Step [3280/4624], Loss: 2.1608, Perplexity: 8.6785\n",
      "Epoch [1/1], Step [3300/4624], Loss: 2.3050, Perplexity: 10.0242\n",
      "Epoch [1/1], Step [3320/4624], Loss: 2.5620, Perplexity: 12.9622\n",
      "Epoch [1/1], Step [3340/4624], Loss: 2.0950, Perplexity: 8.1251\n",
      "Epoch [1/1], Step [3360/4624], Loss: 1.9985, Perplexity: 7.3779\n",
      "Epoch [1/1], Step [3380/4624], Loss: 2.1090, Perplexity: 8.2397\n",
      "Epoch [1/1], Step [3400/4624], Loss: 2.0002, Perplexity: 7.3904\n",
      "Epoch [1/1], Step [3420/4624], Loss: 2.1905, Perplexity: 8.9395\n",
      "Epoch [1/1], Step [3440/4624], Loss: 2.7740, Perplexity: 16.0229\n",
      "Epoch [1/1], Step [3460/4624], Loss: 2.1655, Perplexity: 8.7192\n",
      "Epoch [1/1], Step [3480/4624], Loss: 2.0381, Perplexity: 7.6763\n",
      "Epoch [1/1], Step [3500/4624], Loss: 2.4029, Perplexity: 11.0552\n",
      "Epoch [1/1], Step [3520/4624], Loss: 2.2953, Perplexity: 9.9269\n",
      "Epoch [1/1], Step [3540/4624], Loss: 2.0417, Perplexity: 7.7037\n",
      "Epoch [1/1], Step [3560/4624], Loss: 2.8489, Perplexity: 17.2680\n",
      "Epoch [1/1], Step [3580/4624], Loss: 2.0540, Perplexity: 7.7994\n",
      "Epoch [1/1], Step [3600/4624], Loss: 2.1990, Perplexity: 9.0157\n",
      "Epoch [1/1], Step [3620/4624], Loss: 2.1000, Perplexity: 8.1660\n",
      "Epoch [1/1], Step [3640/4624], Loss: 2.1439, Perplexity: 8.5323\n",
      "Epoch [1/1], Step [3660/4624], Loss: 2.0069, Perplexity: 7.4400\n",
      "Epoch [1/1], Step [3680/4624], Loss: 2.2725, Perplexity: 9.7035\n",
      "Epoch [1/1], Step [3700/4624], Loss: 2.1879, Perplexity: 8.9164\n",
      "Epoch [1/1], Step [3720/4624], Loss: 2.2721, Perplexity: 9.7001\n",
      "Epoch [1/1], Step [3740/4624], Loss: 2.3781, Perplexity: 10.7848\n",
      "Epoch [1/1], Step [3760/4624], Loss: 2.1495, Perplexity: 8.5808\n",
      "Epoch [1/1], Step [3780/4624], Loss: 2.1235, Perplexity: 8.3604\n",
      "Epoch [1/1], Step [3800/4624], Loss: 2.1435, Perplexity: 8.5288\n",
      "Epoch [1/1], Step [3820/4624], Loss: 2.2716, Perplexity: 9.6948\n",
      "Epoch [1/1], Step [3840/4624], Loss: 2.0532, Perplexity: 7.7925\n",
      "Epoch [1/1], Step [3860/4624], Loss: 2.1420, Perplexity: 8.5163\n",
      "Epoch [1/1], Step [3880/4624], Loss: 2.5359, Perplexity: 12.6279\n",
      "Epoch [1/1], Step [3900/4624], Loss: 2.3378, Perplexity: 10.3582\n",
      "Epoch [1/1], Step [3920/4624], Loss: 2.0071, Perplexity: 7.4415\n",
      "Epoch [1/1], Step [3940/4624], Loss: 2.0418, Perplexity: 7.7043\n",
      "Epoch [1/1], Step [3960/4624], Loss: 2.8404, Perplexity: 17.1227\n",
      "Epoch [1/1], Step [3980/4624], Loss: 2.5846, Perplexity: 13.2579\n",
      "Epoch [1/1], Step [4000/4624], Loss: 2.1219, Perplexity: 8.3467\n",
      "Epoch [1/1], Step [4020/4624], Loss: 2.0505, Perplexity: 7.7720\n",
      "Epoch [1/1], Step [4040/4624], Loss: 1.9481, Perplexity: 7.0154\n",
      "Epoch [1/1], Step [4060/4624], Loss: 2.1889, Perplexity: 8.9255\n",
      "Epoch [1/1], Step [4080/4624], Loss: 3.5548, Perplexity: 34.9806\n",
      "Epoch [1/1], Step [4100/4624], Loss: 2.3465, Perplexity: 10.4487\n",
      "Epoch [1/1], Step [4120/4624], Loss: 2.0103, Perplexity: 7.4659\n",
      "Epoch [1/1], Step [4140/4624], Loss: 2.6289, Perplexity: 13.8579\n",
      "Epoch [1/1], Step [4160/4624], Loss: 2.2096, Perplexity: 9.1122\n",
      "Epoch [1/1], Step [4180/4624], Loss: 2.0911, Perplexity: 8.0938\n",
      "Epoch [1/1], Step [4200/4624], Loss: 2.0169, Perplexity: 7.5154\n",
      "Epoch [1/1], Step [4220/4624], Loss: 2.1609, Perplexity: 8.6791\n",
      "Epoch [1/1], Step [4240/4624], Loss: 1.9711, Perplexity: 7.1784\n",
      "Epoch [1/1], Step [4260/4624], Loss: 2.2632, Perplexity: 9.6139\n",
      "Epoch [1/1], Step [4280/4624], Loss: 2.0889, Perplexity: 8.0760\n",
      "Epoch [1/1], Step [4300/4624], Loss: 2.4584, Perplexity: 11.6863\n",
      "Epoch [1/1], Step [4320/4624], Loss: 1.9980, Perplexity: 7.3743\n",
      "Epoch [1/1], Step [4340/4624], Loss: 2.2771, Perplexity: 9.7487\n",
      "Epoch [1/1], Step [4360/4624], Loss: 2.0524, Perplexity: 7.7869\n",
      "Epoch [1/1], Step [4380/4624], Loss: 2.0364, Perplexity: 7.6627\n",
      "Epoch [1/1], Step [4400/4624], Loss: 2.2192, Perplexity: 9.1999\n",
      "Epoch [1/1], Step [4420/4624], Loss: 2.2496, Perplexity: 9.4839\n",
      "Epoch [1/1], Step [4440/4624], Loss: 2.1885, Perplexity: 8.9221\n",
      "Epoch [1/1], Step [4460/4624], Loss: 2.0712, Perplexity: 7.9341\n",
      "Epoch [1/1], Step [4480/4624], Loss: 2.2862, Perplexity: 9.8377\n",
      "Epoch [1/1], Step [4500/4624], Loss: 1.9970, Perplexity: 7.3670\n",
      "Epoch [1/1], Step [4520/4624], Loss: 2.1394, Perplexity: 8.4946\n",
      "Epoch [1/1], Step [4540/4624], Loss: 1.9967, Perplexity: 7.3648\n",
      "Epoch [1/1], Step [4560/4624], Loss: 2.0154, Perplexity: 7.5036\n",
      "Epoch [1/1], Step [4580/4624], Loss: 2.2273, Perplexity: 9.2747\n",
      "Epoch [1/1], Step [4600/4624], Loss: 2.1124, Perplexity: 8.2683\n",
      "Epoch [1/1], Step [4620/4624], Loss: 2.1268, Perplexity: 8.3882\n"
     ]
    }
   ],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join(models_dir, decoder_fname))\n",
    "        torch.save(encoder.state_dict(), os.path.join(models_dir, encoder_fname))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validating the Model using Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Create the data loader.\n",
    "val_data_loader = val_get_loader(\n",
    "    transform=transform_test, \n",
    "    mode=\"valid\", \n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(val_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(11543, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalt 135040\n",
      "-rw-rw-r-- 1 farid farid 41816480 sep 16 23:25 decoder_1_nEpochs.pkl\n",
      "-rw-rw-r-- 1 farid farid 96449530 sep 16 23:25 encoder_1_nEpochs.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -l models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\n",
      "models/encoder_1_nEpochs.pkl\n",
      "models/decoder_1_nEpochs.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(11543, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(models_dir)\n",
    "\n",
    "print(os.path.join(models_dir, encoder_fname))\n",
    "print(os.path.join(models_dir, decoder_fname))\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(models_dir, encoder_fname)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(models_dir, decoder_fname)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# infer captions for all images\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pred_result \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_id, img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img_id, \u001b[38;5;28mtype\u001b[39m(img))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# img = img.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    232\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    print(img_id, type(img))\n",
    "    # img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    #os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    "    os.path.join(cocoapi_dir, \"annotations/captions_val2017.json\"), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad bleu score with only 3 epochs!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
