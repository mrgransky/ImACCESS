			Image 								Text
				↓     								↓
┌───────────────────────────────────────────────────┐
│         CLIP Multi-Modal Architecture             │
│ ┌─────────────┐   ┌───────────────────────────┐   │
│ │ Vision      │   │ Text Transformer          │   │
│ │ Transformer │   │  (context length ~77)     │   │
│ │ (ViT or RN) │   └───────────────────────────┘   │
│ └─────────────┘             ↓                     │
│       ↓        		┌───────────────────────────┐  	│
│  Projection Head	│     Projection Head       │  	│
│    (image)     		│					(text)            │  	│
│       ↓        		└───────────────────────────┘  	│
│       Cross-Modal Embedding Space                	│
│       ↓                			↓                     │
└───────────────────────────────────────────────────┘
								Similarity & Retrieval

Dataset Size:
 - Small: Freeze many blocks, train fewer
 - Large: Train more blocks
Dataset Similarity:
 - Similar domain: Freeze more layers
 - Different domain: Train more layers

Legend:
 - White: Frozen
 - Blue: Trainable
 - Adjust each block in the Vision & Text Transformer to freeze or unfreeze