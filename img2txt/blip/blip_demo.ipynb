{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b949f9f",
   "metadata": {},
   "source": [
    "# BLIP: Inference Demo\n",
    " - [Image Captioning](#Image-Captioning)\n",
    " - [VQA](#VQA)\n",
    " - [Feature Extraction](#Feature-Extraction)\n",
    " - [Image Text Matching](#Image-Text-Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27410f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode, to_pil_image\n",
    "\n",
    "from models.blip import blip_decoder, blip_feature_extractor\n",
    "from models.blip_vqa import blip_vqa\n",
    "from models.blip_itm import blip_itm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb70184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_url='https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' # original photo of BLIP\n",
    "# img_url=\"https://www.thenexttrip.xyz/wp-content/uploads/2022/08/San-Diego-Instagram-Spots-2-820x1025.jpg\" # beach lady looking at the horizon\n",
    "img_url=\"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png\" # singapour\n",
    "# img_url=\"https://www.sunnylife.com.au/cdn/shop/articles/Instagram_1068_1024x1024.jpg\" # beach lady checking phone\n",
    "# img_url=\"https://d3h7nocoh5wnls.cloudfront.net/medium_65f863d2b8a8f574defc0222_Cowgirl_20_Instagram_20_Captions_20_8_e9b3ef13bc.webp\" # standing woman holding rope\n",
    "# img_url=\"https://d3h7nocoh5wnls.cloudfront.net/medium_65f863d2b8a8f574defc058b_One_Word_Joshua_Tree_Captions_0bc104498d.webp\"\n",
    "# img_url=\"https://hips.hearstapps.com/hmg-prod/images/beach-summer-instagram-captions-1621880365.jpg\" # wonderful result\n",
    "# img_url=\"https://company.finnair.com/resource/image/435612/landscape_ratio16x9/1000/563/76f7e18b20ed1612f80937e91235c1a2/C7D5B60FA1B0EDB0ADB9967772AE17C0/history-1924.jpg\"\n",
    "# img_url=\"https://media.istockphoto.com/id/498168409/photo/summer-beach-with-strafish-and-shells.jpg?s=612x612&w=0&k=20&c=_SCAILCSzeekYQQAc94-rlAkj7t_1VmiqOb5DmVo_kE=\"\n",
    "# img_url=\"https://company.finnair.com/resource/image/2213452/landscape_ratio16x9/1000/563/2ffba636bc1b8f612d36fcec5c96420a/3FEFB7C5D68C865BC8CEC368B2728C6E/history-1964.jpg\"\n",
    "# img_url=\"https://company.finnair.com/resource/image/435616/landscape_ratio16x9/1000/563/3e62f054fbb5bb807693d7148286533c/CC6DAD5A4CD3B4D8B3DE10FBEC25073F/history-hero-image.jpg\"\n",
    "# img_url=\"https://company.finnair.com/resource/image/2213582/landscape_ratio16x9/1000/563/35eb282d3ffb3ebde319d072918c7a1a/717BA40152C49614C8073D1F28A0F1A5/history-1983.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c1639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image_tensor, cap=None):\n",
    "  # Step 1: Remove the batch dimension\n",
    "  image_tensor = image_tensor.squeeze(0)  # Now shape is (3, 384, 384)\n",
    "\n",
    "  # Step 2: Permute dimensions from (C, H, W) to (H, W, C)\n",
    "  image_tensor = image_tensor.permute(1, 2, 0)  # Now shape is (384, 384, 3)\n",
    "\n",
    "  # Step 3: Convert to NumPy array\n",
    "  image_np = image_tensor.cpu().detach().numpy()  # Use detach() if the tensor requires grad\n",
    "\n",
    "  # Normalize the image\n",
    "  image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())  # Normalize to [0, 1]\n",
    "    \n",
    "  # Clip values to ensure they are in [0, 1]\n",
    "  image_np = np.clip(image_np, 0, 1)\n",
    "\n",
    "  # Convert to uint8 for visualization\n",
    "  image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "  print(type(image_np), image_np.dtype, image_np.shape, image_np.max(), image_np.min())\n",
    "  \n",
    "  plt.imshow(\n",
    "    X=image_np,\n",
    "    # X=(image_np * 255).astype('uint8'),\n",
    "    # interpolation=None,\n",
    "  )\n",
    "\n",
    "  plt.axis('off')\n",
    "  if cap:\n",
    "    plt.title(cap)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a811a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demo_image(img_source, image_size, device):\n",
    "    raw_image = Image.open(requests.get(img_source, stream=True).raw).convert('RGB')   \n",
    "    w,h = raw_image.size\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(\n",
    "                (image_size,image_size),\n",
    "                interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.48145466, 0.4578275, 0.40821073), \n",
    "                (0.26862954, 0.26130258, 0.27577711),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f4406",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "Perform image captioning using finetuned BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86221da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "image = load_demo_image(\n",
    "  img_source=img_url, \n",
    "  image_size=image_size, \n",
    "  device=device,\n",
    ")\n",
    "print(image.shape, type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d88f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image_tensor=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff0272",
   "metadata": {},
   "source": [
    "# Nucleus Sampling vs. Beam Search: A Comparative Analysis\n",
    "\n",
    "## Nucleus Sampling and Beam Search are two popular decoding strategies used in natural language processing, particularly in tasks like machine translation and image captioning. They both aim to generate the most likely sequence of words given an input (e.g., an image).\n",
    "\n",
    "### Beam Search\n",
    "\n",
    "    Principle: Beam search maintains a fixed-size list of candidate sequences (the beam). At each step, it extends all sequences in the beam by one word, keeping only the top-scoring candidates based on a language model.\n",
    "    Process:\n",
    "        Start with an empty sequence.\n",
    "        Generate all possible next words and calculate their probabilities.\n",
    "        Keep the top-k highest-scoring sequences (the beam).\n",
    "        Repeat steps 2 and 3 until a desired sequence length is reached.\n",
    "    Advantages: Efficient, often produces high-quality results.\n",
    "    Disadvantages: Can be greedy, missing more diverse or creative sequences.\n",
    "\n",
    "### Nucleus Sampling\n",
    "\n",
    "    Principle: Nucleus sampling selects words based on their probabilities, but with a threshold. Only words with probabilities above a certain threshold (nucleus) are considered.\n",
    "    Process:\n",
    "        Generate all possible next words and calculate their probabilities.\n",
    "        Keep only words with probabilities above the nucleus threshold.\n",
    "        Sample a word from the remaining distribution.\n",
    "        Repeat steps 2 and 3 until a desired sequence length is reached.\n",
    "    Advantages: More diverse and creative outputs, can generate less common but still plausible sequences.\n",
    "    Disadvantages: Can be less efficient than beam search, especially with large vocabularies.\n",
    "\n",
    "## Key Differences:\n",
    "\n",
    "    Determinism: Beam search is deterministic, always producing the same sequence given the same input and beam size. Nucleus sampling is stochastic, producing different sequences each time.\n",
    "    Diversity: Nucleus sampling tends to generate more diverse and creative sequences compared to beam search.\n",
    "    Efficiency: Beam search is generally more efficient than nucleus sampling, especially with large beam sizes.\n",
    "\n",
    "## In the context of image captioning:\n",
    "\n",
    "    Beam Search: Often used to generate concise and accurate captions. It's good for tasks where correctness and clarity are prioritized.\n",
    "    Nucleus Sampling: Can be useful for generating more creative or diverse captions, especially when exploring different interpretations of the image.\n",
    "\n",
    "## The choice between beam search and nucleus sampling often depends on the specific requirements of the task and the desired trade-off between efficiency, diversity, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6835daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption():\n",
    "    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'    \n",
    "    model = blip_decoder(\n",
    "        pretrained=model_url, \n",
    "        image_size=image_size, \n",
    "        vit='base',\n",
    "    )\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # # beam search # returns error:\n",
    "        # cap = model.generate(\n",
    "        #     image, \n",
    "        #     sample=False, \n",
    "        #     # num_beams=3, # original implementation\n",
    "        #     num_beams=1, # own implementation\n",
    "        #     max_length=20, \n",
    "        #     min_length=5,\n",
    "        # ) \n",
    "\n",
    "        # nucleus sampling\n",
    "        cap = model.generate(\n",
    "            image, \n",
    "            sample=True, \n",
    "            top_p=0.95, \n",
    "            max_length=20,\n",
    "            min_length=5,\n",
    "        ) \n",
    "    print(f\"{len(cap)} caption(s) generated!\")\n",
    "    # print('caption: '+cap[0])\n",
    "    return cap[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7490f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image_tensor=image, cap=caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0b5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac320a2",
   "metadata": {},
   "source": [
    "# VQA\n",
    "Perform visual question answering using finetuned BLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6f3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 480\n",
    "# image = load_demo_image(\n",
    "#     img_source=img_url,\n",
    "#     image_size=image_size, \n",
    "#     device=device,\n",
    "# )\n",
    "# model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'    \n",
    "# model = blip_vqa(pretrained=model_url, image_size=image_size, vit='base')\n",
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "# question = 'where is the woman sitting?'\n",
    "# with torch.no_grad():\n",
    "#     answer = model(image, question, train=False, inference='generate') \n",
    "#     print('answer: '+answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100e519",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "image = load_demo_image(\n",
    "    img_source=img_url,\n",
    "    image_size=image_size, \n",
    "    device=device,\n",
    ")\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'\n",
    "model = blip_feature_extractor(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "caption = 'a woman sitting on the beach with a dog'\n",
    "multimodal_feature = model(image, caption, mode='multimodal')[0,0]\n",
    "image_feature = model(image, caption, mode='image')[0,0]\n",
    "text_feature = model(image, caption, mode='text')[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e1146",
   "metadata": {},
   "source": [
    "# Image-Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "image = load_demo_image(\n",
    "    img_source=img_url,\n",
    "    image_size=image_size, \n",
    "    device=device,\n",
    ")\n",
    "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
    "\n",
    "model = blip_itm(pretrained=model_url, image_size=image_size, vit='base')\n",
    "model.eval()\n",
    "# model = model.to(device='cpu')\n",
    "model = model.to(device=device)\n",
    "\n",
    "caption = 'a woman sitting on the beach with a dog'\n",
    "print(f'text: {caption}')\n",
    "\n",
    "itm_output = model(image,caption,match_head='itm')\n",
    "\n",
    "itm_score = torch.nn.functional.softmax(itm_output,dim=1)[:,1]\n",
    "print(f'The image and text is matched with a probability of {itm_score}')\n",
    "\n",
    "itc_score = model(image,caption,match_head='itc')\n",
    "print(f'The image feature and text feature has a cosine similarity {itc_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
