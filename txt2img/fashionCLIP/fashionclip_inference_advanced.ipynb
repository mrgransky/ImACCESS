{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 13:08:48.601405: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-01 13:08:48.609955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-01 13:08:48.619539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-01 13:08:48.622630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-01 13:08:48.630169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-01 13:08:49.111299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style <class 'pandas.core.frame.DataFrame'> (44446, 3)\n",
      "      id subCategory  articleType\n",
      "0  15970     Topwear       Shirts\n",
      "1  39386  Bottomwear        Jeans\n",
      "2  59263     Watches      Watches\n",
      "3  21379  Bottomwear  Track Pants\n",
      "4  53759     Topwear      Tshirts\n",
      "5   1855     Topwear      Tshirts\n",
      "6  30805     Topwear       Shirts\n",
      "7  26960     Topwear       Shirts\n",
      "8  29114       Socks        Socks\n",
      "9  30039     Watches      Watches\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('myntradataset/styles.csv', usecols=['id',  'subCategory', 'articleType'])\n",
    "print(f\"Style {type(df)} {df.shape}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = [Image.open(\"outputs/t-shirt.jpeg\")]\n",
    "image = [Image.open(\"outputs/flipflops.jpg\")]\n",
    "# text = [\"hat\", \"t-shirt\", \"shoes\", 'gloves', 'headwear']\n",
    "text = list(set(\n",
    "    list(df[\"articleType\"].unique()) \n",
    "    + list(df[\"subCategory\"].unique())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: torch.Size([1, 172]):\n",
      "tensor([[1.3774e-10, 2.4650e-10, 6.9929e-12, 1.3150e-11, 2.7682e-11, 3.7650e-11,\n",
      "         7.4156e-12, 2.7348e-08, 1.2045e-08, 3.2158e-08, 5.0688e-10, 3.2880e-10,\n",
      "         3.1383e-10, 7.3545e-13, 3.4146e-09, 4.8336e-10, 7.8633e-12, 1.3050e-11,\n",
      "         8.2175e-11, 1.1065e-09, 1.9445e-10, 6.2800e-10, 1.5727e-11, 3.4921e-10,\n",
      "         7.2185e-12, 5.1403e-12, 2.2712e-10, 1.5978e-11, 5.1202e-09, 7.2880e-06,\n",
      "         1.0848e-09, 6.3632e-02, 5.5984e-07, 1.9029e-11, 2.5247e-09, 6.9492e-08,\n",
      "         2.9283e-13, 3.2953e-06, 1.8576e-11, 1.1767e-10, 2.6771e-08, 6.8365e-10,\n",
      "         2.9132e-10, 1.7664e-07, 3.1701e-09, 1.1281e-06, 1.8886e-08, 1.1347e-08,\n",
      "         2.0812e-09, 2.3805e-11, 1.4380e-08, 3.6989e-11, 4.6018e-08, 1.1819e-10,\n",
      "         8.8848e-09, 1.4434e-09, 4.2408e-09, 2.9698e-12, 8.5448e-10, 8.6446e-11,\n",
      "         8.7564e-11, 2.0664e-11, 8.9143e-01, 5.3104e-10, 1.8221e-09, 7.8513e-09,\n",
      "         8.4148e-10, 2.3863e-07, 1.9958e-12, 5.1662e-09, 3.2063e-13, 3.4630e-12,\n",
      "         1.7739e-06, 2.1109e-10, 1.1523e-10, 1.1651e-07, 2.1030e-10, 4.1595e-14,\n",
      "         1.0200e-09, 2.0053e-10, 8.5823e-07, 4.1409e-09, 1.2977e-09, 1.9291e-11,\n",
      "         3.9895e-08, 6.1701e-08, 6.4782e-09, 4.1783e-02, 3.7689e-07, 1.9282e-12,\n",
      "         9.9666e-11, 3.0663e-10, 1.3030e-07, 2.0351e-11, 2.7358e-07, 2.5568e-12,\n",
      "         8.2877e-12, 5.2105e-10, 1.2084e-10, 4.2459e-11, 1.1127e-09, 3.5896e-08,\n",
      "         1.6810e-12, 5.0888e-08, 2.5721e-08, 8.1472e-07, 8.2258e-09, 1.0960e-13,\n",
      "         1.3364e-11, 2.9565e-05, 3.0336e-08, 1.2053e-10, 5.2163e-11, 8.7622e-08,\n",
      "         5.3498e-10, 2.9251e-03, 1.2660e-06, 6.0256e-05, 2.9796e-08, 2.6647e-08,\n",
      "         1.7391e-09, 1.5337e-09, 1.5128e-10, 1.4207e-10, 4.7448e-10, 1.0802e-08,\n",
      "         1.7525e-09, 3.9357e-10, 4.8824e-11, 2.8752e-07, 6.9122e-11, 1.5026e-09,\n",
      "         3.1881e-10, 5.2792e-08, 1.8420e-09, 3.0837e-08, 3.3420e-08, 5.3154e-05,\n",
      "         3.4164e-11, 5.1127e-11, 3.7948e-10, 3.1018e-10, 3.2331e-09, 2.9317e-08,\n",
      "         2.9079e-10, 6.1750e-14, 1.6228e-06, 2.4579e-09, 5.9760e-10, 5.0978e-07,\n",
      "         6.5542e-12, 2.7840e-10, 2.7971e-10, 5.7299e-10, 4.9835e-10, 2.7519e-07,\n",
      "         6.4535e-12, 6.1498e-05, 1.2824e-12, 1.5650e-10, 3.7704e-10, 7.1111e-12,\n",
      "         1.5115e-10, 1.1770e-08, 1.9884e-08, 9.5078e-09, 1.7655e-08, 5.6778e-10,\n",
      "         2.5498e-11, 2.4506e-10, 2.0068e-07, 1.8047e-09]])\n",
      "62 Flip Flops\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained('Marqo/marqo-fashionCLIP', trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained('Marqo/marqo-fashionCLIP', trust_remote_code=True)\n",
    "processed = processor(text=text, images=image, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(processed['pixel_values'], normalize=True)\n",
    "    text_features = model.get_text_features(processed['input_ids'], normalize=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(f\"Label probs: {text_probs.shape}:\\n{text_probs}\")\n",
    "top_result = np.argmax(text_probs).item()\n",
    "print(top_result, text[top_result])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
