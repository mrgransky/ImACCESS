{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = AutoModel.from_pretrained('Marqo/marqo-fashionCLIP', trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained('Marqo/marqo-fashionCLIP', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style <class 'pandas.core.frame.DataFrame'> (44446, 3)\n",
      "      id subCategory  articleType\n",
      "0  15970     Topwear       Shirts\n",
      "1  39386  Bottomwear        Jeans\n",
      "2  59263     Watches      Watches\n",
      "3  21379  Bottomwear  Track Pants\n",
      "4  53759     Topwear      Tshirts\n",
      "5   1855     Topwear      Tshirts\n",
      "6  30805     Topwear       Shirts\n",
      "7  26960     Topwear       Shirts\n",
      "8  29114       Socks        Socks\n",
      "9  30039     Watches      Watches\n",
      "143 Classes:\n",
      "['Accessory Gift Set' 'Baby Dolls' 'Backpacks' 'Bangle' 'Basketballs'\n",
      " 'Bath Robe' 'Beauty Accessory' 'Belts' 'Blazers' 'Body Lotion'\n",
      " 'Body Wash and Scrub' 'Booties' 'Boxers' 'Bra' 'Bracelet' 'Briefs'\n",
      " 'Camisoles' 'Capris' 'Caps' 'Casual Shoes' 'Churidar' 'Clothing Set'\n",
      " 'Clutches' 'Compact' 'Concealer' 'Cufflinks' 'Cushion Covers' 'Deodorant'\n",
      " 'Dresses' 'Duffel Bag' 'Dupatta' 'Earrings' 'Eye Cream' 'Eyeshadow'\n",
      " 'Face Moisturisers' 'Face Scrub and Exfoliator' 'Face Serum and Gel'\n",
      " 'Face Wash and Cleanser' 'Flats' 'Flip Flops' 'Footballs' 'Formal Shoes'\n",
      " 'Foundation and Primer' 'Fragrance Gift Set' 'Free Gifts' 'Gloves'\n",
      " 'Hair Accessory' 'Hair Colour' 'Handbags' 'Hat' 'Headband' 'Heels'\n",
      " 'Highlighter and Blush' 'Innerwear Vests' 'Ipad' 'Jackets' 'Jeans'\n",
      " 'Jeggings' 'Jewellery Set' 'Jumpsuit' 'Kajal and Eyeliner' 'Key chain'\n",
      " 'Kurta Sets' 'Kurtas' 'Kurtis' 'Laptop Bag' 'Leggings' 'Lehenga Choli'\n",
      " 'Lip Care' 'Lip Gloss' 'Lip Liner' 'Lip Plumper' 'Lipstick'\n",
      " 'Lounge Pants' 'Lounge Shorts' 'Lounge Tshirts' 'Makeup Remover'\n",
      " 'Mascara' 'Mask and Peel' 'Mens Grooming Kit' 'Messenger Bag'\n",
      " 'Mobile Pouch' 'Mufflers' 'Nail Essentials' 'Nail Polish'\n",
      " 'Necklace and Chains' 'Nehru Jackets' 'Night suits' 'Nightdress'\n",
      " 'Patiala' 'Pendant' 'Perfume and Body Mist' 'Rain Jacket' 'Rain Trousers'\n",
      " 'Ring' 'Robe' 'Rompers' 'Rucksacks' 'Salwar' 'Salwar and Dupatta'\n",
      " 'Sandals' 'Sarees' 'Scarves' 'Shapewear' 'Shirts' 'Shoe Accessories'\n",
      " 'Shoe Laces' 'Shorts' 'Shrug' 'Skirts' 'Socks' 'Sports Sandals'\n",
      " 'Sports Shoes' 'Stockings' 'Stoles' 'Suits' 'Sunglasses' 'Sunscreen'\n",
      " 'Suspenders' 'Sweaters' 'Sweatshirts' 'Swimwear' 'Tablet Sleeve' 'Ties'\n",
      " 'Ties and Cufflinks' 'Tights' 'Toner' 'Tops' 'Track Pants' 'Tracksuits'\n",
      " 'Travel Accessory' 'Trolley Bag' 'Trousers' 'Trunk' 'Tshirts' 'Tunics'\n",
      " 'Umbrellas' 'Waist Pouch' 'Waistcoat' 'Wallets' 'Watches' 'Water Bottle'\n",
      " 'Wristbands']\n",
      "[ 111   16  724   85   13   20    4  813    8    6    1   12   52  477\n",
      "   66  849   39  175  283 2846   30    8  290   49   11  106    1  347\n",
      "  464   88  116  417    6   42   61    5    2   28  500  916    8  637\n",
      "   76   57   91   20    1   19 1759    3    7 1323   53  242    1  258\n",
      "  609   34   58   16  102    2   94 1844  234   82  177    4   16  144\n",
      "   48    4  315   61   34    3    4   13   12    1   44   47   38    6\n",
      "  329  160    5  141  189   38  176  614   18    2  118    4   12   11\n",
      "   32    7  897  427  119    9 3217   23    1  547    6  128  686   67\n",
      " 2036   32   90    1 1073   25   40  277  285   17    3  263    2    9\n",
      "    5 1762  304   29   16    3  530  140 7070  229    6   17   15  936\n",
      " 2542   11    7]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('myntradataset/styles.csv', usecols=['id',  'subCategory', 'articleType'])\n",
    "print(f\"Style {type(df)} {df.shape}\")\n",
    "print(df.head(10))\n",
    "\n",
    "unique, counts = np.unique(df[\"articleType\"].tolist(), return_counts=True)\n",
    "print(f\"{unique.shape[0]} Classes:\\n{unique}\\n{counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[1.7655e-08, 1.5026e-09, 2.6647e-08, 7.1111e-12, 1.5650e-10, 1.8047e-09,\n",
      "         2.9565e-05, 4.1409e-09, 5.1403e-12, 5.1202e-09, 1.9282e-12, 8.8848e-09,\n",
      "         3.2063e-13, 8.7564e-11, 1.3774e-10, 8.9143e-01, 6.5542e-12, 3.9357e-10,\n",
      "         7.4156e-12, 3.5896e-08, 8.2258e-09, 6.9122e-11, 3.1701e-09, 2.7519e-07,\n",
      "         2.7358e-07, 1.0848e-09, 3.1018e-10, 1.4207e-10, 7.2185e-12, 1.5128e-10,\n",
      "         1.3050e-11, 5.0978e-07, 3.2331e-09, 7.2880e-06, 2.3863e-07, 4.6018e-08,\n",
      "         3.4164e-11, 1.1347e-08, 1.5727e-11, 2.3805e-11, 9.5078e-09, 2.4650e-10,\n",
      "         8.2877e-12, 5.2105e-10, 4.9835e-10, 3.0663e-10, 1.5115e-10, 5.0888e-08,\n",
      "         2.9796e-08, 2.0812e-09, 1.8221e-09, 6.2800e-10, 1.3150e-11, 1.0802e-08,\n",
      "         3.7689e-07, 3.0336e-08, 9.9666e-11, 6.3632e-02, 3.2158e-08, 5.2163e-11,\n",
      "         2.8752e-07, 2.9698e-12, 1.0200e-09, 6.8365e-10, 2.4579e-09, 2.7840e-10,\n",
      "         2.9283e-13, 4.2408e-09, 1.9445e-10, 1.2660e-06, 2.0053e-10, 1.3030e-07,\n",
      "         1.9884e-08, 1.1770e-08, 3.2953e-06, 8.7622e-08, 3.1383e-10, 1.8576e-11,\n",
      "         2.0351e-11, 1.7739e-06, 3.0837e-08, 1.1819e-10, 4.1783e-02, 3.7704e-10,\n",
      "         1.1127e-09, 2.9317e-08, 5.3104e-10, 2.1030e-10, 4.8824e-11, 1.1767e-10,\n",
      "         1.2824e-12, 3.7948e-10, 2.6771e-08, 1.8420e-09, 5.3498e-10, 1.7525e-09,\n",
      "         1.2053e-10, 1.8886e-08, 4.1595e-14, 3.7650e-11, 2.1109e-10, 3.1881e-10,\n",
      "         4.7448e-10, 5.6778e-10, 3.6989e-11, 1.2045e-08, 8.2175e-11, 2.0664e-11,\n",
      "         4.8336e-10, 6.1750e-14, 5.1127e-11, 1.6810e-12, 6.1701e-08, 8.1472e-07,\n",
      "         1.4434e-09, 1.3364e-11, 5.7299e-10, 3.4630e-12, 1.5978e-11, 3.9895e-08,\n",
      "         1.9958e-12, 3.4921e-10, 2.7682e-11, 8.6446e-11, 6.9929e-12, 1.1281e-06,\n",
      "         2.7971e-10, 2.0068e-07, 1.6228e-06, 7.8513e-09, 2.5568e-12, 2.5498e-11,\n",
      "         3.4146e-09, 2.7348e-08, 6.9492e-08, 6.4782e-09, 1.1523e-10, 1.9029e-11,\n",
      "         1.1065e-09, 4.2459e-11, 1.5337e-09, 1.7391e-09, 2.9079e-10, 1.7664e-07,\n",
      "         7.3545e-13, 2.9251e-03, 8.5448e-10, 6.1498e-05, 6.0256e-05, 8.5823e-07,\n",
      "         1.0960e-13, 2.5721e-08, 5.3154e-05, 5.0688e-10, 3.2880e-10, 3.3420e-08,\n",
      "         1.9291e-11, 8.4148e-10, 2.4506e-10, 5.2792e-08, 2.2712e-10, 5.5984e-07,\n",
      "         6.4535e-12, 1.4380e-08, 7.8633e-12, 5.1662e-09, 5.9760e-10, 2.9132e-10,\n",
      "         2.5247e-09, 1.2084e-10, 1.1651e-07, 1.2977e-09]])\n",
      "15 Flip Flops\n"
     ]
    }
   ],
   "source": [
    "# image = [Image.open(\"outputs/t-shirt.jpeg\")]\n",
    "image = [Image.open(\"outputs/flipflops.jpg\")]\n",
    "# text = [\"hat\", \"t-shirt\", \"shoes\", 'gloves', 'headwear']\n",
    "text = list(set(\n",
    "    list(df[\"articleType\"].unique()) \n",
    "    + list(df[\"subCategory\"].unique())\n",
    "))\n",
    "\n",
    "processed = processor(text=text, images=image, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(processed['pixel_values'], normalize=True)\n",
    "    text_features = model.get_text_features(processed['input_ids'], normalize=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)\n",
    "# [0.99990773, 0.00006382, 0.00002847]\n",
    "top_result = np.argmax(text_probs).item()\n",
    "print(top_result, text[top_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "HOME: str = os.getenv('HOME') # echo $HOME\n",
    "USER: str = os.getenv('USER') # echo $USER\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_models = [\n",
    "\t\"openai/clip-vit-base-patch32\", # original\n",
    "\t\"openai/clip-vit-large-patch14\",\n",
    "\t\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "]\n",
    "\n",
    "if USER==\"farid\": # local laptop\n",
    "\tWDIR = os.path.join(HOME, \"datasets\")\n",
    "\tmodels_dir = os.path.join(WDIR, \"trash\", \"models\")\n",
    "\tmodel_fpth = pretrained_models[0]\n",
    "elif USER==\"alijanif\": # Puhti\n",
    "\tWDIR = \"/scratch/project_2004072/ImACCESS\"\n",
    "\tmodels_dir = os.path.join(WDIR, \"trash\", \"models\")\n",
    "\tmodel_fpth = pretrained_models[1]\n",
    "else: # Pouta\n",
    "\tWDIR = \"/media/volume/ImACCESS\"\n",
    "\tmodels_dir = os.path.join(HOME, WDIR, \"models\")\n",
    "\tmodel_fpth = pretrained_models[1]\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(model_fpth, cache_dir=models_dir)\n",
    "processor = CLIPProcessor.from_pretrained(model_fpth, cache_dir=models_dir)\n",
    "\n",
    "# Load and preprocess images\n",
    "# \"https://www.archives.gov/files/research/military/ww2/photos/images/ww2-14.jpg\"\n",
    "base_url = \"https://www.archives.gov/files/research/military/ww2/photos/images/ww2-\"\n",
    "image_urls = [f\"{base_url}{i:02d}.jpg\" for i in range(1, 203)]\n",
    "print(f\"len(image_urls): {len(image_urls)}\")\n",
    "\n",
    "images = []\n",
    "for url in image_urls:\n",
    "  response = requests.get(url)\n",
    "  img = Image.open(BytesIO(response.content))\n",
    "  images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode images and text\n",
    "text = \"Cemetery\"\n",
    "inputs = processor(text=[text], images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Compute similarity scores\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "print(logits_per_image.shape, type(logits_per_image))\n",
    "probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n",
    "print(probs.shape, type(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure top_k is not greater than the number of images\n",
    "top_k = min(3, len(images))\n",
    "top_k_indices = probs.topk(top_k, dim=0).indices.squeeze().tolist()\n",
    "print(top_k_indices, type(top_k_indices))\n",
    "# Handle the case where top_k_indices is not a list (e.g., when there's only one image)\n",
    "if isinstance(top_k_indices, int):\n",
    "  top_k_indices = [top_k_indices]\n",
    "\n",
    "top_k_images = [images[i] for i in top_k_indices]\n",
    "print(top_k_images) # [<PIL.JpegImagePlugin.JpegImageFile image mode=L size=1100x1396 at 0x78A40EF4B910>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=1104x1387 at 0x78A40EF4BBB0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=1437x1066 at 0x78A40EA22730>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, top_k, figsize=(15, 5))\n",
    "for i, img in enumerate(top_k_images):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
