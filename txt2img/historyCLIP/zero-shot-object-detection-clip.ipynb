{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Object Localization and Detection with OpenAI's CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading the dataset. We are using the <a ref= https://huggingface.co/datasets/jamescalam/image-text-demo> image-text-demo </a> from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset #pip install datasets\n",
    "from PIL import Image, ImageDraw, ImageOps, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from absl import logging as absl_logging\n",
    "\n",
    "# Set environment variables to suppress TensorFlow and oneDNN warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Configure logging for TensorFlow and related libraries\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('xla').setLevel(logging.ERROR)\n",
    "logging.getLogger('cuda').setLevel(logging.ERROR)\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress absl logging\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)\n",
    "absl_logging.set_stderrthreshold(absl_logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "\t\"jamescalam/image-text-demo\",\n",
    "\tsplit=\"train\",\n",
    "\trevision=\"180fdae\",\n",
    "\ttrust_remote_code=True  # Add this line to trust the remote code\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes 21 labelled images. We chose the 3rd image in the dataset, *butterfly landing on the nose of a cat*, to perform our object localization task. Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "query_image = 2\n",
    "data[query_image]['image']\n",
    "# label\n",
    "data[query_image]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, lbl in zip(data['image'], data['text']):\n",
    "\tprint(lbl)\n",
    "\t# display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break Image into Equal Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists of transforming our image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# transform the image into tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "img = transform(data[query_image][\"image\"])\n",
    "# img = transform(Image.open(\"/home/farid/WS_Farid/ImACCESS/TEST_IMGs/cat_butterfly.jpg\"))\n",
    "img.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated tensor has $3$ color channels, a height of $5184$, and width of $3456$\n",
    "\n",
    "H,W = 5184,3456\n",
    "\n",
    "We now want to add an extra dimension, which will be needed for later calculations. We can use the `unfold` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra dimension for later calculations\n",
    "patches = img.data.unfold(0,3,3)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now break the image into patches. More precisely, we want to break the image into $256x256$ pixels patches. We start by breaking it horizontally, meaning that we will end up with an image composed of $20$ patches of $256$ pixels in height, and $1$ patch of $3456$ pixels in width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the image into patches (in height dimension)\n",
    "patch = 256\n",
    "\n",
    "patches = patches.unfold(1, patch, patch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = patches.shape[1]\n",
    "\n",
    "fig, ax = plt.subplots(X, 1, figsize=(15, 15))\n",
    "# loop through each strip and display\n",
    "for x in range(X):\n",
    "\tprint(x)\n",
    "\tprint(patches[0, x].permute(2, 0, 1).shape)\n",
    "\tax[x].imshow(patches[0, x].permute(2, 0, 1))\n",
    "\tax[x].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `unfold` function again to break the image vertically. After this operation, we will get an image composed of $20x13$ patches of $256x256$ pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the image into patches (in width dimension)\n",
    "patches = patches.unfold(2, patch, patch)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = patches.shape[1]\n",
    "Y = patches.shape[2]\n",
    "\n",
    "fig, ax = plt.subplots(X, Y, figsize=(Y*2, X*2))\n",
    "for x in range(X):\n",
    "\tfor y in range(Y):\n",
    "\t\tax[x, y].imshow(patches[0, x, y].permute(1, 2, 0))\n",
    "\t\tax[x, y].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Patches using CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is done. We are now almost ready to process those patches using CLIP. Before doing it, we might want to work through these patches by grouping them into a 6x6 window.\n",
    "\n",
    "<center><div> <img src=\"https://raw.githubusercontent.com/pinecone-io/examples/master/learn/image-retrieval/clip-object-detection/assets/window.png\" alt=\"Drawing\" style=\"width:300px;\"/></div> </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the 6x6 window\n",
    "window = 6\n",
    "\n",
    "big_patch = torch.zeros(patch*window, patch*window, 3)\n",
    "patch_batch = patches[0][:window][:window]\n",
    "\n",
    "# visualize patch\n",
    "for y in range(window):\n",
    "\tfor x in range(window):\n",
    "\t\tbig_patch[y*patch:(y+1)*patch, x*patch:(x+1)*patch, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\n",
    "plt.imshow(big_patch)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a patch consisting of 6x6 smaller patches.\n",
    "\n",
    "We can repeat this process by \"sliding\" the 6x6 window over the full image. We set the stride, i.e., the number of steps the window moves, to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 6\n",
    "# stride = 1\n",
    "\n",
    "# # window slides from top to bottom\n",
    "# for Y in range(0, patches.shape[1]-window+1, stride):\n",
    "#     # window slides from left to right\n",
    "#     for X in range(0, patches.shape[2]-window+1, stride):\n",
    "#         # initialize an empty big_patch array\n",
    "#         big_patch = torch.zeros(patch*window, patch*window, 3)\n",
    "\n",
    "#         # this gets the current batch of patches that will make big_batch\n",
    "#         patch_batch = patches[0, Y:Y+window, X:X+window]\n",
    "#         # loop through each patch in current batch\n",
    "#         for y in range(patch_batch.shape[1]):\n",
    "#             for x in range(patch_batch.shape[0]):\n",
    "#                 # add patch to big_patch\n",
    "#                 big_patch[\n",
    "#                     y*patch:(y+1)*patch, x*patch:(x+1)*patch, :\n",
    "#                 ] = patch_batch[y, x].permute(1, 2, 0)\n",
    "#         # display current big_patch\n",
    "#         plt.imshow(big_patch)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to process these through CLIP and calculate the similarity between the patch and a prompt. Our first prompt will be `\"a fluffy cat\"`. Let's first define our processor and model CLIP and move it to device, if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define processor and model\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# move model to device if possible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add a step to the previous operation, i.e., we calculate similarity scores for each patch (`scores`) and the number of time the window slides over each patch (`runs`). To do that, we are trasmitting to the CLIP model the image, and the prompt, which is \"a fluffy cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 6\n",
    "stride = 1\n",
    "\n",
    "scores = torch.zeros(patches.shape[1], patches.shape[2])\n",
    "runs = torch.ones(patches.shape[1], patches.shape[2])\n",
    "\n",
    "for Y in range(0, patches.shape[1]-window+1, stride):\n",
    "\tfor X in range(0, patches.shape[2]-window+1, stride):\n",
    "\t\tbig_patch = torch.zeros(patch*window, patch*window, 3)\n",
    "\t\tpatch_batch = patches[0, Y:Y+window, X:X+window]\n",
    "\t\tfor y in range(window):\n",
    "\t\t\tfor x in range(window):\n",
    "\t\t\t\tbig_patch[\n",
    "\t\t\t\t\ty*patch:(y+1)*patch, x*patch:(x+1)*patch, :\n",
    "\t\t\t\t] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\t# we preprocess the image and class label with the CLIP processor\n",
    "\t\tinputs = processor(\n",
    "\t\t\timages=big_patch,  # big patch image sent to CLIP\n",
    "\t\t\treturn_tensors=\"pt\",  # tell CLIP to return pytorch tensor\n",
    "\t\t\ttext=\"a fluffy cat\",  # class label sent to CLIP\n",
    "\t\t\tpadding=True\n",
    "\t\t).to(device) # move to device if possible\n",
    "\n",
    "\t\t# calculate and retrieve similarity score\n",
    "\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\t# sum up similarity scores from current and previous big patches\n",
    "\t\t# that were calculated for patches within the current window\n",
    "\t\tscores[Y:Y+window, X:X+window] += score\n",
    "\t\t# calculate the number of runs on each patch within the current window\n",
    "\t\truns[Y:Y+window, X:X+window] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to divide the total score (`scores`) by the number of time the window slided over the patch (`runs`) to get an average score for each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average score for each patch\n",
    "scores /= runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial visual is not very useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the patches tensor \n",
    "adj_patches = patches.squeeze(0).permute(3, 4, 2, 0, 1)\n",
    "# normalize scores\n",
    "scores = (\n",
    "\tscores - scores.min()) / (scores.max() - scores.min()\n",
    ")\n",
    "# multiply patches by scores\n",
    "adj_patches = adj_patches * scores\n",
    "# rotate patches to visualize\n",
    "adj_patches = adj_patches.permute(3, 4, 2, 0, 1)\n",
    "\n",
    "Y = adj_patches.shape[0]\n",
    "X = adj_patches.shape[1]\n",
    "\n",
    "fig, ax = plt.subplots(Y, X, figsize=(X*.5, Y*.5))\n",
    "for y in range(Y):\n",
    "\tfor x in range(X):\n",
    "\t\tax[y, x].imshow(adj_patches[y, x].permute(1, 2, 0))\n",
    "\t\tax[y, x].axis(\"off\")\n",
    "\t\tax[y, x].set_aspect('equal')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor is characterized by a smooth gradient of scores and little-to-no impact from the scores on our visual. That is why (1) we are going to clip the score's interval edges for each patch so that they are around the average score, and (2) normalize scores for each patch. \n",
    "\n",
    "Using the `numpy.clip()` function, we can set all the scores below the average score equal to $0$ while keeping those higher than the average score as they are.\n",
    "\n",
    "We are running the same operation 3 times as it seems to give us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip the scores' interval edges\n",
    "for _ in range(1):\n",
    "\tscores = np.clip(scores-scores.mean(), 0, np.inf)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we normalize the scores using the min-max normalization. For every tensor, the minimum value is transformed into a 0, the maximum value into a 1, and every other value into a decimal between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize scores\n",
    "scores = (\n",
    "\tscores - scores.min()) / (scores.max() - scores.min()\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are telling us if a given patch contains \"a fluffy cat\" or not. The higher the score, the more the probability that the cat is localized in that patch. \n",
    "\n",
    "We now want to visualize those scores, i.e., the localized object, on the original image. Scores equal to zero will be represented by black patches, so that the localized object can be clearly seen.\n",
    "\n",
    "To do that, we can multiply our scores by the patches. This requires that scores and patches have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given they do not have the same shape, we can transform the patches shape using `squeeze` and `permute`. Squeeze reduce the dimensionality, while permute rotates the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the patches tensor \n",
    "adj_patches = patches.squeeze(0).permute(3, 4, 2, 0, 1)\n",
    "adj_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now multiply patches to scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply patches by scores\n",
    "adj_patches = adj_patches * scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting the localized object, we can rotate the patch tensor again to make our life easier ahead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate patches to visualize\n",
    "adj_patches = adj_patches.permute(3, 4, 2, 0, 1)\n",
    "adj_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the localized object. We are expecting to visualize the cat only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = adj_patches.shape[0]\n",
    "X = adj_patches.shape[1]\n",
    "\n",
    "fig, ax = plt.subplots(Y, X, figsize=(X*.5, Y*.5))\n",
    "for y in range(Y):\n",
    "\tfor x in range(X):\n",
    "\t\tax[y, x].imshow(adj_patches[y, x].permute(1, 2, 0))\n",
    "\t\tax[y, x].axis(\"off\")\n",
    "\t\tax[y, x].set_aspect('equal')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked pretty well! \n",
    "\n",
    "Now let's do the same for butterfly... in this case, the text trasmitted to the CLIP model will be \"a butterfly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 6\n",
    "stride = 1\n",
    "\n",
    "scores = torch.zeros(patches.shape[1], patches.shape[2])\n",
    "runs = torch.ones(patches.shape[1], patches.shape[2])\n",
    "\n",
    "for Y in range(0, patches.shape[1]-window, stride):\n",
    "\tfor X in range(0, patches.shape[2]-window, stride):\n",
    "\t\tbig_patch = torch.zeros(patch*window, patch*window, 3)\n",
    "\t\tpatch_batch = patches[0, Y:Y+window, X:X+window]\n",
    "\t\tfor y in range(window):\n",
    "\t\t\tfor x in range(window):\n",
    "\t\t\t\tbig_patch[y*patch:(y+1)*patch, x*patch:(x+1)*patch, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\tinputs = processor(\n",
    "\t\t\timages=big_patch,\n",
    "\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\ttext=\"a butterfly\",\n",
    "\t\t\tpadding=True\n",
    "\t\t).to(device)\n",
    "\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\tscores[Y:Y+window, X:X+window] += score\n",
    "\t\truns[Y:Y+window, X:X+window] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores /= runs\n",
    "for _ in range(3):\n",
    "\tscores = np.clip(scores-scores.mean(), 0, np.inf)\n",
    "# normalize scores\n",
    "scores = (scores - scores.min()) / (scores.max() - scores.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust patches\n",
    "adj_patches = patches.squeeze(0).permute(3, 4, 2, 0, 1) * scores\n",
    "adj_patches = adj_patches.permute(3, 4, 2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = adj_patches.shape[0]\n",
    "X = adj_patches.shape[1]\n",
    "\n",
    "fig, ax = plt.subplots(Y, X, figsize=(X*.5, Y*.5))\n",
    "for y in range(Y):\n",
    "\tfor x in range(X):\n",
    "\t\tax[y, x].imshow(adj_patches[y, x].permute(1, 2, 0))\n",
    "\t\tax[y, x].axis(\"off\")\n",
    "\t\tax[y, x].set_aspect('equal')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP for Object Detection\n",
    "\n",
    "Localization is one step towards object detection, where we might expect to detect multiple objects.\n",
    "\n",
    "We can extend our current localization code quite easily to achieve this, we just add an extra layer of logic. But, before we do anything, we need to rethink our localization visual — this would be hard to represent when displaying two or more objects.\n",
    "\n",
    "The typical approach to this is to create a \"bounding box\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores we are using are from the last run, with which we localized the butterly. Therefore, we are going to build the a bounding box around the butterfly first. We want the bounding box to focus as much as possible on the butterly only, nothing else.\n",
    "\n",
    "Scores higher than $0.5$ seem to give us a more precise bounding box. We have then defined 'detection', which gives us *True* when the score is higher than $0.5$ (i.e.,non-zero positions), and *False* otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores higher than 0.5\n",
    "detection = scores > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now detect the non-zero *positions* with the `np.nonzero` function.  These represent the co-ordinates of our patches with scores $>0.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-zero positions\n",
    "np.nonzero(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min, y_max = (\n",
    "\tnp.nonzero(detection)[:,0].min().item(),\n",
    "\tnp.nonzero(detection)[:,0].max().item()+1\n",
    ")\n",
    "y_min, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = (\n",
    "\tnp.nonzero(detection)[:,1].min().item(),\n",
    "\tnp.nonzero(detection)[:,1].max().item()+1\n",
    ")\n",
    "x_min, x_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These give us the bounding box corner co-ordinates based on patches rather than pixel values. To get the pixel co-ordinates we need to multiply by the `patch` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min *= patch\n",
    "y_max *= patch\n",
    "x_min *= patch\n",
    "x_max *= patch\n",
    "x_min, y_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `(y_max - y_min)` and `(x_max - x_min)` to calculate the height and width of the bounding box respectively..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = y_max - y_min\n",
    "width = x_max - x_min\n",
    "\n",
    "height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our patches are $256x256$ pixels, we obtain a total height and width of $256*8=2048$ and $256*4=1024$ pixels, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be now be able to visualize the bounding box on our image. We are going to use 'matplotlib'. This need the image's color channel to be as last in the image's shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.data.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, this is in the first position. We then need to move it to end. We are using 'moveaxis' to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move color channel to final dim\n",
    "image = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(Y*0.5, X*0.5))\n",
    "ax.imshow(image)\n",
    "# Create a Rectangle patch\n",
    "rect = mpl.patches.Rectangle(\n",
    "\t(x_min, y_min),\n",
    "\twidth,\n",
    "\theight,\n",
    "\tlinewidth=2,\n",
    "\tedgecolor='#FAFF00', \n",
    "\tfacecolor='none',\n",
    "\talpha=0.8,\n",
    ")\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this process for a number of objects that we'd like CLIP to detect. let's put everything we've done so far in a few helper functions, then create a new function called `detect` to handle the detection of multiple objects and visualization of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "\t'#FF00FF',\n",
    "\t'#00FF00',\n",
    "\t'#FAFF00',\n",
    "\t'#8CF1FF',\n",
    "\t'#FF0000',\n",
    "\t'#0000FF',\n",
    "\t'#000000',\n",
    "\t'#FFFFFF',\n",
    "\t'#808080',\n",
    "\t'#800000',\n",
    "\t'#808000',\n",
    "\t'#008000',\n",
    "\t'#008080',\n",
    "\t'#000080',\n",
    "\t'#800080',\n",
    "\t'#FFA500',\n",
    "\t'#FFC0CB',\n",
    "\t'#FFD700',\n",
    "\t'#FF69B4',\n",
    "\t'#FF4500',\n",
    "\t'#FF1493',\n",
    "\t'#FF00FF',\n",
    "\t'#FF0000',\n",
    "\t'#FF00FF',\n",
    "]\n",
    "\n",
    "def get_patches(img, patch_size=256):\n",
    "\t# add extra dimension for later calculations\n",
    "\timg_patches = img.data.unfold(0,3,3)\n",
    "\t# break the image into patches (in height dimension)\n",
    "\timg_patches = img_patches.unfold(1, patch_size, patch_size)\n",
    "\t# break the image into patches (in width dimension)\n",
    "\timg_patches = img_patches.unfold(2, patch_size, patch_size)\n",
    "\treturn img_patches\n",
    "\n",
    "def get_scores(img_patches, prompt, window=6, stride=1):\n",
    "\t# initialize scores and runs arrays\n",
    "\tscores = torch.zeros(img_patches.shape[1], img_patches.shape[2])\n",
    "\truns = torch.ones(img_patches.shape[1], img_patches.shape[2])\n",
    "\n",
    "\t# iterate through patches\n",
    "\tfor Y in range(0, img_patches.shape[1]-window+1, stride):\n",
    "\t\tfor X in range(0, img_patches.shape[2]-window+1, stride):\n",
    "\t\t\t# initialize array to store big patches\n",
    "\t\t\tbig_patch = torch.zeros(patch*window, patch*window, 3)\n",
    "\t\t\t# get a single big patch\n",
    "\t\t\tpatch_batch = img_patches[0, Y:Y+window, X:X+window]\n",
    "\t\t\t# iteratively build all big patches\n",
    "\t\t\tfor y in range(window):\n",
    "\t\t\t\tfor x in range(window):\n",
    "\t\t\t\t\tbig_patch[y*patch:(y+1)*patch, x*patch:(x+1)*patch, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\t\tinputs = processor(\n",
    "\t\t\t\timages=big_patch, # image trasmitted to the model\n",
    "\t\t\t\treturn_tensors=\"pt\", # return pytorch tensor\n",
    "\t\t\t\ttext=prompt, # text trasmitted to the model\n",
    "\t\t\t\tpadding=True\n",
    "\t\t\t).to(device) # move to device if possible\n",
    "\n",
    "\t\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\t\t# sum up similarity scores\n",
    "\t\t\tscores[Y:Y+window, X:X+window] += score\n",
    "\t\t\t# calculate the number of runs \n",
    "\t\t\truns[Y:Y+window, X:X+window] += 1\n",
    "\t# calculate average scores\n",
    "\tscores /= runs\n",
    "\t# clip scores\n",
    "\tfor _ in range(3):\n",
    "\t\tscores = np.clip(scores-scores.mean(), 0, np.inf)\n",
    "\t# normalize scores\n",
    "\tscores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\tprint(type(scores), scores.shape, scores.min(), scores.max())\n",
    "\treturn scores\n",
    "\n",
    "def get_box(scores, patch_size=256, threshold=0.5):\n",
    "\tdetection = scores > threshold\n",
    "\t# find box corners\n",
    "\ty_min, y_max = np.nonzero(detection)[:,0].min().item(), np.nonzero(detection)[:,0].max().item()+1\n",
    "\tx_min, x_max = np.nonzero(detection)[:,1].min().item(), np.nonzero(detection)[:,1].max().item()+1\n",
    "\t# convert from patch co-ords to pixel co-ords\n",
    "\ty_min *= patch_size\n",
    "\ty_max *= patch_size\n",
    "\tx_min *= patch_size\n",
    "\tx_max *= patch_size\n",
    "\t# calculate box height and width\n",
    "\theight = y_max - y_min\n",
    "\twidth = x_max - x_min\n",
    "\treturn x_min, y_min, width, height\n",
    "\n",
    "def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5):\n",
    "\t# build image patches for detection\n",
    "\timg_patches = get_patches(img, patch_size)\n",
    "\t# convert image to format for displaying with matplotlib\n",
    "\timage = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "\t# initialize plot to display image + bounding boxes\n",
    "\tfig, ax = plt.subplots(figsize=(10, 10))\n",
    "\tax.imshow(image)\n",
    "\t# process image through object detection steps\n",
    "\tfor i, prompt in enumerate(tqdm(prompts)):\n",
    "\t\tscores = get_scores(img_patches, prompt, window, stride)\n",
    "\t\t# Check if there's any detection above the threshold\n",
    "\t\tdetection = scores.numpy() > threshold\n",
    "\t\tif np.any(detection):\n",
    "\t\t\tx, y, width, height = get_box(scores, patch_size, threshold)\n",
    "\t\t\t# Only add the rectangle if the detection meets the threshold\n",
    "\t\t\trect = mpl.patches.Rectangle(\n",
    "\t\t\t\t(x, y), \n",
    "\t\t\t\twidth, \n",
    "\t\t\t\theight, \n",
    "\t\t\t\tlinewidth=1.5,\n",
    "\t\t\t\talpha=0.8,\n",
    "\t\t\t\tedgecolor=colors[i], \n",
    "\t\t\t\tfacecolor='none',\n",
    "\t\t\t)\n",
    "\t\t\tax.add_patch(rect)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"No {prompt} detected in the image.\")\n",
    "\tax.axis('off')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"dog\", \"ball\", \"cat\", \"butterfly\", \"car\", \"apple\", \"banana\", \"orange\", \"bird\", \"flower\"]\n",
    "detect(labels, img, window=4, stride=1, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "colors = [\n",
    "\t\t'#0000FF', '#000000', '#FFFFFF', '#808080', '#800000',\n",
    "\t\t'#808000', '#008000', '#008080', '#000080', '#800080',\n",
    "\t\t'#FF00FF', '#00FF00', '#FAFF00', '#8CF1FF', '#FF0000',\n",
    "\t\t'#FFA500', '#FFC0CB', '#FFD700', '#FF69B4', '#FF4500',\n",
    "\t\t'#FF1493', '#FF00FF', '#FF0000', '#FF00FF',\n",
    "]\n",
    "\n",
    "def get_patches(img, patch_size=256):\n",
    "\t\timg_patches = img.data.unfold(0, 3, 3)\n",
    "\t\timg_patches = img_patches.unfold(1, patch_size, patch_size)\n",
    "\t\timg_patches = img_patches.unfold(2, patch_size, patch_size)\n",
    "\t\treturn img_patches\n",
    "\n",
    "def get_scores(img_patches, prompt, patch_size, window=6, stride=1):\n",
    "\t\tscores = torch.zeros(img_patches.shape[1], img_patches.shape[2])\n",
    "\t\truns = torch.ones(img_patches.shape[1], img_patches.shape[2])\n",
    "\n",
    "\t\tfor Y in range(0, img_patches.shape[1] - window + 1, stride):\n",
    "\t\t\t\tfor X in range(0, img_patches.shape[2] - window + 1, stride):\n",
    "\t\t\t\t\t\tbig_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
    "\t\t\t\t\t\tpatch_batch = img_patches[0, Y:Y + window, X:X + window]\n",
    "\t\t\t\t\t\tfor y in range(window):\n",
    "\t\t\t\t\t\t\t\tfor x in range(window):\n",
    "\t\t\t\t\t\t\t\t\t\tbig_patch[y * patch_size:(y + 1) * patch_size, x * patch_size:(x + 1) * patch_size, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\t\t\t\t\tinputs = processor(\n",
    "\t\t\t\t\t\t\t\timages=big_patch,\n",
    "\t\t\t\t\t\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\t\t\t\t\t\ttext=prompt,\n",
    "\t\t\t\t\t\t\t\tpadding=True\n",
    "\t\t\t\t\t\t).to(device)\n",
    "\n",
    "\t\t\t\t\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\t\t\t\t\tscores[Y:Y + window, X:X + window] += score\n",
    "\t\t\t\t\t\truns[Y:Y + window, X:X + window] += 1\n",
    "\n",
    "\t\tscores /= runs\n",
    "\t\tfor _ in range(3):\n",
    "\t\t\t\tscores = np.clip(scores - scores.mean(), 0, np.inf)\n",
    "\t\tscores = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\t\treturn scores\n",
    "\n",
    "def get_box(scores, patch_size=256, threshold=0.5):\n",
    "\t\t# Convert scores to a NumPy array\n",
    "\t\tscores_np = scores.numpy()\n",
    "\t\tdetection = scores_np > threshold\n",
    "\t\tif np.any(detection):\n",
    "\t\t\t\t# Unpack the tuple returned by np.nonzero\n",
    "\t\t\t\ty_indices, x_indices = np.nonzero(detection)\n",
    "\t\t\t\ty_min, y_max = y_indices.min().item(), y_indices.max().item() + 1\n",
    "\t\t\t\tx_min, x_max = x_indices.min().item(), x_indices.max().item() + 1\n",
    "\t\t\t\ty_min *= patch_size\n",
    "\t\t\t\ty_max *= patch_size\n",
    "\t\t\t\tx_min *= patch_size\n",
    "\t\t\t\tx_max *= patch_size\n",
    "\t\t\t\theight = y_max - y_min\n",
    "\t\t\t\twidth = x_max - x_min\n",
    "\t\t\t\treturn x_min, y_min, width, height\n",
    "\t\treturn None\n",
    "\n",
    "def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5):\n",
    "\t\timg_patches = get_patches(img, patch_size)\n",
    "\t\timage = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "\t\tfig, ax = plt.subplots(figsize=(10, 10))\n",
    "\t\tax.imshow(image)\n",
    "\n",
    "\t\tfor i, prompt in enumerate(tqdm(prompts)):\n",
    "\t\t\t\tscores = get_scores(img_patches, prompt, patch_size, window, stride)\n",
    "\t\t\t\tbox = get_box(scores, patch_size, threshold)\n",
    "\t\t\t\tif box:\n",
    "\t\t\t\t\t\tx, y, width, height = box\n",
    "\t\t\t\t\t\trect = mpl_patches.Rectangle(\n",
    "\t\t\t\t\t\t\t\t(x, y),\n",
    "\t\t\t\t\t\t\t\twidth,\n",
    "\t\t\t\t\t\t\t\theight,\n",
    "\t\t\t\t\t\t\t\tlinewidth=1.5,\n",
    "\t\t\t\t\t\t\t\talpha=0.8,\n",
    "\t\t\t\t\t\t\t\tedgecolor=colors[i],\n",
    "\t\t\t\t\t\t\t\tfacecolor='none',\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\tax.add_patch(rect)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint(f\"No {prompt} detected in the image.\")\n",
    "\n",
    "\t\tax.axis('off')\n",
    "\t\tplt.savefig(\"output.png\")\n",
    "\t\t# plt.show()\n",
    "\n",
    "# Example usage\n",
    "labels = [\"dog\", \"ball\", \"cat\", \"butterfly\", \"car\", \"apple\", \"banana\", \"orange\", \"bird\", \"flower\"]\n",
    "detect(labels, img, window=4, stride=1, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "colors = [\n",
    "\t'#FF00FF', '#00FF00', '#FAFF00', '#8CF1FF', '#FF0000',\n",
    "\t'#0000FF', '#000000', '#FFFFFF', '#808080', '#800000',\n",
    "\t'#808000', '#008000', '#008080', '#000080', '#800080',\n",
    "\t'#FFA500', '#FFC0CB', '#FFD700', '#FF69B4', '#FF4500',\n",
    "]\n",
    "\n",
    "def get_patches(img, patch_size=256):\n",
    "\timg_patches = img.data.unfold(0, 3, 3)\n",
    "\timg_patches = img_patches.unfold(1, patch_size, patch_size)\n",
    "\timg_patches = img_patches.unfold(2, patch_size, patch_size)\n",
    "\treturn img_patches\n",
    "\n",
    "def get_scores(img_patches, prompt, patch_size, window=6, stride=1):\n",
    "\tscores = torch.zeros(img_patches.shape[1], img_patches.shape[2])\n",
    "\truns = torch.ones(img_patches.shape[1], img_patches.shape[2])\n",
    "\t\n",
    "\tfor Y in range(0, img_patches.shape[1] - window + 1, stride):\n",
    "\t\tfor X in range(0, img_patches.shape[2] - window + 1, stride):\n",
    "\t\t\tbig_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
    "\t\t\tpatch_batch = img_patches[0, Y:Y + window, X:X + window]\n",
    "\t\t\t\n",
    "\t\t\tfor y in range(window):\n",
    "\t\t\t\tfor x in range(window):\n",
    "\t\t\t\t\tbig_patch[y * patch_size:(y + 1) * patch_size, \n",
    "\t\t\t\t\t\t\tx * patch_size:(x + 1) * patch_size, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\t\t\n",
    "\t\t\tinputs = processor(\n",
    "\t\t\t\timages=big_patch,\n",
    "\t\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\t\ttext=prompt,\n",
    "\t\t\t\tpadding=True\n",
    "\t\t\t).to(device)\n",
    "\t\t\t\n",
    "\t\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\t\tscores[Y:Y + window, X:X + window] += score\n",
    "\t\t\truns[Y:Y + window, X:X + window] += 1\n",
    "\t\n",
    "\tscores /= runs\n",
    "\treturn scores\n",
    "\n",
    "def normalize_scores(scores):\n",
    "\t\"\"\"Normalize scores with improved contrast and filtering.\"\"\"\n",
    "\tscores_np = scores.numpy()\n",
    "\t\n",
    "\t# Apply Gaussian smoothing to reduce noise\n",
    "\tfrom scipy.ndimage import gaussian_filter\n",
    "\tscores_np = gaussian_filter(scores_np, sigma=1.0)\n",
    "\t\n",
    "\t# Enhanced contrast normalization\n",
    "\tscores_np = np.clip(scores_np - scores_np.mean(), 0, np.inf)\n",
    "\tscores_np = (scores_np - scores_np.min()) / (scores_np.max() - scores_np.min() + 1e-8)\n",
    "\t\n",
    "\treturn torch.from_numpy(scores_np)\n",
    "\n",
    "def get_box(scores, patch_size=256, threshold=0.5, min_area=4):\n",
    "\t\"\"\"Get bounding box with minimum area requirement and confidence score.\"\"\"\n",
    "\tscores_np = scores.numpy()\n",
    "\tdetection = scores_np > threshold\n",
    "\t\n",
    "\tif np.sum(detection) < min_area:  # Minimum area threshold\n",
    "\t\treturn None, 0.0\n",
    "\t\n",
    "\tif np.any(detection):\n",
    "\t\ty_indices, x_indices = np.nonzero(detection)\n",
    "\t\ty_min, y_max = y_indices.min().item(), y_indices.max().item() + 1\n",
    "\t\tx_min, x_max = x_indices.min().item(), x_indices.max().item() + 1\n",
    "\t\t\n",
    "\t\t# Calculate confidence as mean score in the detected region\n",
    "\t\tconfidence = float(scores_np[y_min:y_max, x_min:x_max].mean())\n",
    "\t\t\n",
    "\t\t# Convert to image coordinates\n",
    "\t\ty_min *= patch_size\n",
    "\t\ty_max *= patch_size\n",
    "\t\tx_min *= patch_size\n",
    "\t\tx_max *= patch_size\n",
    "\t\t\n",
    "\t\theight = y_max - y_min\n",
    "\t\twidth = x_max - x_min\n",
    "\t\t\n",
    "\t\treturn (x_min, y_min, width, height), confidence\n",
    "\t\n",
    "\treturn None, 0.0\n",
    "\n",
    "def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5, conf_threshold=0.3):\n",
    "\t\"\"\"Improved detection with confidence filtering and legend.\"\"\"\n",
    "\timg_patches = get_patches(img, patch_size)\n",
    "\timage = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "\t\n",
    "\tfig, ax = plt.subplots(figsize=(12, 8))\n",
    "\tax.imshow(image)\n",
    "\t\n",
    "\t# Store detected objects for legend\n",
    "\tdetected_objects = []\n",
    "\t\n",
    "\tfor i, prompt in enumerate(tqdm(prompts)):\n",
    "\t\tscores = get_scores(img_patches, prompt, patch_size, window, stride)\n",
    "\t\tscores = normalize_scores(scores)\n",
    "\t\tbox, confidence = get_box(scores, patch_size, threshold)\n",
    "\t\t\n",
    "\t\tif box and confidence > conf_threshold:\n",
    "\t\t\tx, y, width, height = box\n",
    "\t\t\trect = mpl_patches.Rectangle(\n",
    "\t\t\t\t(x, y),\n",
    "\t\t\t\twidth,\n",
    "\t\t\t\theight,\n",
    "\t\t\t\tlinewidth=2,\n",
    "\t\t\t\talpha=0.8,\n",
    "\t\t\t\tedgecolor=colors[i % len(colors)],\n",
    "\t\t\t\tfacecolor='none',\n",
    "\t\t\t\tlabel=f\"{prompt} ({confidence:.2f})\"\n",
    "\t\t\t)\n",
    "\t\t\tax.add_patch(rect)\n",
    "\t\t\tdetected_objects.append((prompt, confidence))\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"No {prompt} detected with sufficient confidence.\")\n",
    "\t\n",
    "\t# Add legend if objects were detected\n",
    "\tif detected_objects:\n",
    "\t\tax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\t\n",
    "\tax.axis('off')\n",
    "\tplt.tight_layout()\n",
    "\treturn detected_objects\n",
    "\n",
    "# Example usage\n",
    "labels = [\"dog\", \"ball\", \"cat\", \"butterfly\", \"car\", \"apple\", \"banana\", \"orange\", \"bird\", \"flower\"]\n",
    "detected = detect(labels, img, \n",
    "\t\t\t\t window=4, \n",
    "\t\t\t\t stride=1, \n",
    "\t\t\t\t threshold=0.8,\n",
    "\t\t\t\t conf_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "colors = [\n",
    "\t\t'#FF00FF', '#00FF00', '#FAFF00', '#8CF1FF', '#FF0000',\n",
    "\t\t'#0000FF', '#000000', '#FFFFFF', '#808080', '#800000',\n",
    "\t\t'#808000', '#008000', '#008080', '#000080', '#800080',\n",
    "\t\t'#FFA500', '#FFC0CB', '#FFD700', '#FF69B4', '#FF4500',\n",
    "]\n",
    "\n",
    "def get_patches(img, patch_size=256):\n",
    "\t\timg_patches = img.data.unfold(0, 3, 3)\n",
    "\t\timg_patches = img_patches.unfold(1, patch_size, patch_size)\n",
    "\t\timg_patches = img_patches.unfold(2, patch_size, patch_size)\n",
    "\t\treturn img_patches\n",
    "\n",
    "def get_initial_score(img, prompt):\n",
    "\t\t\"\"\"Get initial whole-image relevance score for the prompt.\"\"\"\n",
    "\t\tinputs = processor(\n",
    "\t\t\t\timages=img,\n",
    "\t\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\t\ttext=prompt,\n",
    "\t\t\t\tpadding=True\n",
    "\t\t).to(device)\n",
    "\t\t\n",
    "\t\treturn model(**inputs).logits_per_image.item()\n",
    "\n",
    "def get_scores(img_patches, prompt, patch_size, window=6, stride=1):\n",
    "\t\tscores = torch.zeros(img_patches.shape[1], img_patches.shape[2])\n",
    "\t\truns = torch.ones(img_patches.shape[1], img_patches.shape[2])\n",
    "\t\t\n",
    "\t\tfor Y in range(0, img_patches.shape[1] - window + 1, stride):\n",
    "\t\t\t\tfor X in range(0, img_patches.shape[2] - window + 1, stride):\n",
    "\t\t\t\t\t\tbig_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
    "\t\t\t\t\t\tpatch_batch = img_patches[0, Y:Y + window, X:X + window]\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tfor y in range(window):\n",
    "\t\t\t\t\t\t\t\tfor x in range(window):\n",
    "\t\t\t\t\t\t\t\t\t\tbig_patch[y * patch_size:(y + 1) * patch_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tx * patch_size:(x + 1) * patch_size, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tinputs = processor(\n",
    "\t\t\t\t\t\t\t\timages=big_patch,\n",
    "\t\t\t\t\t\t\t\treturn_tensors=\"pt\",\n",
    "\t\t\t\t\t\t\t\ttext=prompt,\n",
    "\t\t\t\t\t\t\t\tpadding=True\n",
    "\t\t\t\t\t\t).to(device)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tscore = model(**inputs).logits_per_image.item()\n",
    "\t\t\t\t\t\tscores[Y:Y + window, X:X + window] += score\n",
    "\t\t\t\t\t\truns[Y:Y + window, X:X + window] += 1\n",
    "\t\t\n",
    "\t\tscores /= runs\n",
    "\t\treturn scores\n",
    "\n",
    "def normalize_scores(scores):\n",
    "\t\t\"\"\"Normalize scores with improved contrast and filtering.\"\"\"\n",
    "\t\tscores_np = scores.numpy()\n",
    "\t\t\n",
    "\t\t# Apply Gaussian smoothing to reduce noise\n",
    "\t\tfrom scipy.ndimage import gaussian_filter\n",
    "\t\tscores_np = gaussian_filter(scores_np, sigma=1.0)\n",
    "\t\t\n",
    "\t\t# Enhanced contrast normalization\n",
    "\t\tscores_np = np.clip(scores_np - scores_np.mean(), 0, np.inf)\n",
    "\t\tscores_np = (scores_np - scores_np.min()) / (scores_np.max() - scores_np.min() + 1e-8)\n",
    "\t\t\n",
    "\t\treturn torch.from_numpy(scores_np)\n",
    "\n",
    "def get_box(scores, patch_size=256, threshold=0.5, min_area=4):\n",
    "\t\t\"\"\"Get bounding box with minimum area requirement and confidence score.\"\"\"\n",
    "\t\tscores_np = scores.numpy()\n",
    "\t\tdetection = scores_np > threshold\n",
    "\t\t\n",
    "\t\tif np.sum(detection) < min_area:  # Minimum area threshold\n",
    "\t\t\t\treturn None, 0.0\n",
    "\t\t\n",
    "\t\tif np.any(detection):\n",
    "\t\t\t\ty_indices, x_indices = np.nonzero(detection)\n",
    "\t\t\t\ty_min, y_max = y_indices.min().item(), y_indices.max().item() + 1\n",
    "\t\t\t\tx_min, x_max = x_indices.min().item(), x_indices.max().item() + 1\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Calculate confidence as mean score in the detected region\n",
    "\t\t\t\tconfidence = float(scores_np[y_min:y_max, x_min:x_max].mean())\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Convert to image coordinates\n",
    "\t\t\t\ty_min *= patch_size\n",
    "\t\t\t\ty_max *= patch_size\n",
    "\t\t\t\tx_min *= patch_size\n",
    "\t\t\t\tx_max *= patch_size\n",
    "\t\t\t\t\n",
    "\t\t\t\theight = y_max - y_min\n",
    "\t\t\t\twidth = x_max - x_min\n",
    "\t\t\t\t\n",
    "\t\t\t\treturn (x_min, y_min, width, height), confidence\n",
    "\t\t\n",
    "\t\treturn None, 0.0\n",
    "\n",
    "def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.5, relevance_threshold=0.2):\n",
    "\t\t\"\"\"Improved detection with automatic relevance filtering.\"\"\"\n",
    "\t\timg_patches = get_patches(img, patch_size)\n",
    "\t\timage = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "\t\t\n",
    "\t\t# First pass: check whole-image relevance for each prompt\n",
    "\t\trelevant_prompts = []\n",
    "\t\trelevance_scores = []\n",
    "\t\t\n",
    "\t\tprint(\"Checking image-level relevance for each label...\")\n",
    "\t\tfor prompt in tqdm(prompts):\n",
    "\t\t\t\trelevance = get_initial_score(img, prompt)\n",
    "\t\t\t\tif relevance > relevance_threshold:\n",
    "\t\t\t\t\t\trelevant_prompts.append(prompt)\n",
    "\t\t\t\t\t\trelevance_scores.append(relevance)\n",
    "\t\t\n",
    "\t\tif not relevant_prompts:\n",
    "\t\t\t\tprint(\"No relevant objects detected in the image.\")\n",
    "\t\t\t\treturn []\n",
    "\t\t\n",
    "\t\t# Sort prompts by relevance score\n",
    "\t\tsorted_prompts = [x for _, x in sorted(zip(relevance_scores, relevant_prompts), reverse=True)]\n",
    "\t\t\n",
    "\t\tfig, ax = plt.subplots(figsize=(12, 8))\n",
    "\t\tax.imshow(image)\n",
    "\t\t\n",
    "\t\tdetected_objects = []\n",
    "\t\t\n",
    "\t\tprint(\"\\nDetecting and localizing relevant objects...\")\n",
    "\t\tfor i, prompt in enumerate(tqdm(sorted_prompts)):\n",
    "\t\t\t\tscores = get_scores(img_patches, prompt, patch_size, window, stride)\n",
    "\t\t\t\tscores = normalize_scores(scores)\n",
    "\t\t\t\tbox, confidence = get_box(scores, patch_size, threshold)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif box:\n",
    "\t\t\t\t\t\tx, y, width, height = box\n",
    "\t\t\t\t\t\trect = mpl_patches.Rectangle(\n",
    "\t\t\t\t\t\t\t\t(x, y),\n",
    "\t\t\t\t\t\t\t\twidth,\n",
    "\t\t\t\t\t\t\t\theight,\n",
    "\t\t\t\t\t\t\t\tlinewidth=2,\n",
    "\t\t\t\t\t\t\t\talpha=0.8,\n",
    "\t\t\t\t\t\t\t\tedgecolor=colors[i % len(colors)],\n",
    "\t\t\t\t\t\t\t\tfacecolor='none',\n",
    "\t\t\t\t\t\t\t\tlabel=f\"{prompt} ({confidence:.2f})\"\n",
    "\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\tax.add_patch(rect)\n",
    "\t\t\t\t\t\tdetected_objects.append((prompt, confidence))\n",
    "\t\t\n",
    "\t\tif detected_objects:\n",
    "\t\t\t\tax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\t\t\n",
    "\t\tax.axis('off')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(\"output_detection.png\")\n",
    "\t\t\n",
    "\t\t# Print summary\n",
    "\t\tprint(\"\\nDetection Summary:\")\n",
    "\t\tprint(f\"Total labels checked: {len(prompts)}\")\n",
    "\t\tprint(f\"Relevant labels found: {len(relevant_prompts)}\")\n",
    "\t\tprint(f\"Objects localized: {len(detected_objects)}\")\n",
    "\t\t\n",
    "\t\treturn detected_objects\n",
    "\n",
    "# Example usage with all labels\n",
    "labels = [\"dog\", \"ball\", \"cat\", \"butterfly\", \"car\", \"apple\", \"banana\", \"orange\", \"bird\", \"flower\"]\n",
    "detected = detect(\n",
    "\tlabels, \n",
    "\timg, \n",
    "\twindow=4, \n",
    "\tstride=1, \n",
    "\tthreshold=0.86, # Confidence threshold for bounding boxes\n",
    "\trelevance_threshold=0.95, # Relevance threshold for initial detection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpl_patches\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "colors = [\n",
    "    '#FF00FF', '#00FF00', '#FAFF00', '#8CF1FF', '#FF0000',\n",
    "    '#0000FF', '#000000', '#FFFFFF', '#808080', '#800000',\n",
    "]\n",
    "\n",
    "def get_patches(img, patch_size=256):\n",
    "    img_patches = img.data.unfold(0, 3, 3)\n",
    "    img_patches = img_patches.unfold(1, patch_size, patch_size)\n",
    "    img_patches = img_patches.unfold(2, patch_size, patch_size)\n",
    "    return img_patches\n",
    "\n",
    "def get_initial_score(img, prompt):\n",
    "    \"\"\"Get initial whole-image relevance score with multiple viewpoints.\"\"\"\n",
    "    # Check the whole image\n",
    "    inputs_full = processor(\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        text=prompt,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    full_score = model(**inputs_full).logits_per_image.item()\n",
    "    \n",
    "    # Check negative prompt to establish baseline\n",
    "    negative_prompt = f\"an image without a {prompt}\"\n",
    "    inputs_neg = processor(\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        text=negative_prompt,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    neg_score = model(**inputs_neg).logits_per_image.item()\n",
    "    \n",
    "    # Calculate relative confidence\n",
    "    relative_confidence = full_score / (full_score + neg_score + 1e-6)\n",
    "    \n",
    "    return relative_confidence\n",
    "\n",
    "def get_scores(img_patches, prompt, patch_size, window=6, stride=1):\n",
    "    scores = torch.zeros(img_patches.shape[1], img_patches.shape[2])\n",
    "    runs = torch.ones(img_patches.shape[1], img_patches.shape[2])\n",
    "    \n",
    "    # Add negative checking for each patch\n",
    "    negative_prompt = f\"an image without a {prompt}\"\n",
    "    \n",
    "    for Y in range(0, img_patches.shape[1] - window + 1, stride):\n",
    "        for X in range(0, img_patches.shape[2] - window + 1, stride):\n",
    "            big_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
    "            patch_batch = img_patches[0, Y:Y + window, X:X + window]\n",
    "            \n",
    "            for y in range(window):\n",
    "                for x in range(window):\n",
    "                    big_patch[y * patch_size:(y + 1) * patch_size, \n",
    "                            x * patch_size:(x + 1) * patch_size, :] = patch_batch[y, x].permute(1, 2, 0)\n",
    "            \n",
    "            # Check positive prompt\n",
    "            inputs_pos = processor(\n",
    "                images=big_patch,\n",
    "                return_tensors=\"pt\",\n",
    "                text=prompt,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            pos_score = model(**inputs_pos).logits_per_image.item()\n",
    "            \n",
    "            # Check negative prompt\n",
    "            inputs_neg = processor(\n",
    "                images=big_patch,\n",
    "                return_tensors=\"pt\",\n",
    "                text=negative_prompt,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            neg_score = model(**inputs_neg).logits_per_image.item()\n",
    "            \n",
    "            # Calculate relative confidence\n",
    "            relative_score = pos_score / (pos_score + neg_score + 1e-6)\n",
    "            \n",
    "            scores[Y:Y + window, X:X + window] += relative_score\n",
    "            runs[Y:Y + window, X:X + window] += 1\n",
    "    \n",
    "    scores /= runs\n",
    "    return scores\n",
    "\n",
    "def verify_detection(img, box, prompt, threshold):\n",
    "    \"\"\"Verify detection by checking the specific region.\"\"\"\n",
    "    if box is None:\n",
    "        return False, 0.0\n",
    "    \n",
    "    x, y, width, height = box\n",
    "    region = img[:, y:y+height, x:x+width]\n",
    "    \n",
    "    # Check positive prompt\n",
    "    inputs_pos = processor(\n",
    "        images=region,\n",
    "        return_tensors=\"pt\",\n",
    "        text=prompt,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    pos_score = model(**inputs_pos).logits_per_image.item()\n",
    "    \n",
    "    # Check negative prompt\n",
    "    negative_prompt = f\"an image without a {prompt}\"\n",
    "    inputs_neg = processor(\n",
    "        images=region,\n",
    "        return_tensors=\"pt\",\n",
    "        text=negative_prompt,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    neg_score = model(**inputs_neg).logits_per_image.item()\n",
    "    \n",
    "    # Calculate final confidence\n",
    "    final_confidence = pos_score / (pos_score + neg_score + 1e-6)\n",
    "    \n",
    "    return final_confidence > threshold, final_confidence\n",
    "\n",
    "def detect(prompts, img, patch_size=256, window=6, stride=1, threshold=0.86, relevance_threshold=0.95):\n",
    "    \"\"\"Improved detection with robust filtering and verification.\"\"\"\n",
    "    img_patches = get_patches(img, patch_size)\n",
    "    image = np.moveaxis(img.data.numpy(), 0, -1)\n",
    "    \n",
    "    # First pass: check whole-image relevance\n",
    "    relevant_prompts = []\n",
    "    relevance_scores = []\n",
    "    \n",
    "    print(\"Checking image-level relevance for each label...\")\n",
    "    for prompt in tqdm(prompts):\n",
    "        relevance = get_initial_score(img, prompt)\n",
    "        if relevance > relevance_threshold:\n",
    "            relevant_prompts.append(prompt)\n",
    "            relevance_scores.append(relevance)\n",
    "    \n",
    "    if not relevant_prompts:\n",
    "        print(\"No relevant objects detected in the image.\")\n",
    "        return []\n",
    "    \n",
    "    # Sort prompts by relevance\n",
    "    sorted_pairs = sorted(zip(relevance_scores, relevant_prompts), reverse=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    detected_objects = []\n",
    "    \n",
    "    print(\"\\nDetecting and verifying objects...\")\n",
    "    for score, prompt in tqdm(sorted_pairs):\n",
    "        scores = get_scores(img_patches, prompt, patch_size, window, stride)\n",
    "        scores = gaussian_filter(scores.numpy(), sigma=1.0)\n",
    "        scores = torch.from_numpy(scores)\n",
    "        \n",
    "        # Get initial box\n",
    "        box, confidence = get_box(scores, patch_size, threshold)\n",
    "        \n",
    "        # Verify detection\n",
    "        if box is not None:\n",
    "            is_valid, final_confidence = verify_detection(img, box, prompt, threshold)\n",
    "            \n",
    "            if is_valid:\n",
    "                x, y, width, height = box\n",
    "                rect = mpl_patches.Rectangle(\n",
    "                    (x, y),\n",
    "                    width,\n",
    "                    height,\n",
    "                    linewidth=2,\n",
    "                    alpha=0.8,\n",
    "                    edgecolor=colors[len(detected_objects) % len(colors)],\n",
    "                    facecolor='none',\n",
    "                    label=f\"{prompt} ({final_confidence:.2f})\"\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                detected_objects.append((prompt, final_confidence))\n",
    "    \n",
    "    if detected_objects:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nDetection Summary:\")\n",
    "    print(f\"Total labels checked: {len(prompts)}\")\n",
    "    print(f\"Objects detected and verified: {len(detected_objects)}\")\n",
    "    for obj, conf in detected_objects:\n",
    "        print(f\"- {obj}: {conf:.2f}\")\n",
    "    \n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with all labels\n",
    "labels = [\"dog\", \"ball\", \"cat\", \"butterfly\", \"car\", \"apple\", \"banana\", \"orange\", \"bird\", \"flower\"]\n",
    "detected = detect(\n",
    "\tlabels,\n",
    "\timg, \n",
    "\twindow=4, \n",
    "\tstride=1, \n",
    "\tthreshold=0.46, # Confidence threshold for bounding boxes\n",
    "\trelevance_threshold=0.45, # Relevance threshold for initial detection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
